{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-BiLSTM-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\ProgramData\\anaconda3\\envs\\GIS\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\anaconda3\\envs\\GIS\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\anaconda3\\envs\\GIS\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "from pandarallel import pandarallel\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import trange, tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchcrf import CRF\n",
    "from torch.nn import LSTM\n",
    "\n",
    "# 初始化pandarallel\n",
    "pandarallel.initialize(nb_workers=psutil.cpu_count(logical=False))\n",
    "\n",
    "# 读取数据\n",
    "df_label = pd.read_csv('data\\WeiboTrainData\\weibo_label.csv')\n",
    "\n",
    "# 定义数据清洗函数\n",
    "def clean_content(content):\n",
    "    import re\n",
    "    content = re.sub(r'分享图片|分享视频|微博视频|的微博视频|#|，|,|。|；|、|！|\\+|=|-|&|:|%|\\?', '', content)\n",
    "    content = re.sub(r'[：\"\\'/|……]', '', content)\n",
    "    content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.', '')\n",
    "    content = re.sub(r'[_\\[\\]【】<>《》（）\\(\\)]', '', content)\n",
    "    content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "    return content\n",
    "\n",
    "# 清洗数据\n",
    "df_label['message'] = df_label['message'].parallel_apply(clean_content)\n",
    "\n",
    "# 去除空的行\n",
    "df_label= df_label[df_label['message'] != '']\n",
    "\n",
    "# 保留text文本长度大于4的行\n",
    "df_label = df_label[df_label['message'].str.len() > 4]\n",
    "\n",
    "# 数据预处理\n",
    "df_label.sentiment.value_counts()\n",
    "df = df_label.copy()\n",
    "df.loc[df[df.sentiment!=6].index,'sentiment'] = 1\n",
    "df = df[(df['sentiment']==1) | (df['sentiment']==6)]\n",
    "drop_size = len(df[df['sentiment']==1].sentiment) - len(df[df['sentiment']==6].sentiment)\n",
    "df.drop(df[df['sentiment']==1].sample(drop_size).index, inplace=True)\n",
    "df.loc[df[df.sentiment==6].index,'sentiment'] = 0\n",
    "df.sentiment.unique()\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['message'], df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# 加载BERT模型和分词器\n",
    "model_name = 'bert-base-chinese'\n",
    "config = BertConfig.from_pretrained('model/' + model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained('model/' + model_name)\n",
    "bert_model = BertForSequenceClassification.from_pretrained('model/' + model_name, num_labels=2)\n",
    "# bert_model = BertForSequenceClassification.from_pretrained('model/' + model_name, num_labels=768)\n",
    "\n",
    "# 定义BERT-BiLSTM-CRF模型\n",
    "class BertBiLSTMCRF(torch.nn.Module):                                      \n",
    "    def __init__(self, bert_model, num_labels, hidden_dim):\n",
    "        super(BertBiLSTMCRF, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.lstm = LSTM(input_size=bert_model.config.hidden_size, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim * 2, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        lstm_output, _ = self.lstm(sequence_output)\n",
    "        emissions = self.fc(lstm_output)\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(emissions, labels, mask=attention_mask.byte())\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(emissions, mask=attention_mask.byte())\n",
    "\n",
    "# model = BertBiLSTMCRF(bert_model, num_labels=2, hidden_dim=256)\n",
    "model = BertBiLSTMCRF(bert_model, num_labels=3, hidden_dim=256)\n",
    "\n",
    "# 定义分词函数\n",
    "def get_tokens(text, tokenizer, max_seq_length, add_special_tokens=True):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=add_special_tokens, truncation=True, max_length=max_seq_length, pad_to_max_length=True)\n",
    "    attention_mask = [int(id > 0) for id in input_ids]\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(attention_mask) == max_seq_length\n",
    "    return (input_ids, attention_mask)\n",
    "\n",
    "# 对训练集和测试集进行分词\n",
    "X_train_tokens = X_train.apply(get_tokens, args=(tokenizer, 150))\n",
    "X_test_tokens = X_test.apply(get_tokens, args=(tokenizer, 150))\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "input_ids_train = torch.tensor([features[0] for features in X_train_tokens.values], dtype=torch.long)\n",
    "input_mask_train = torch.tensor([features[1] for features in X_train_tokens.values], dtype=torch.long)\n",
    "label_ids_train = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "\n",
    "input_ids_test = torch.tensor([features[0] for features in X_test_tokens.values], dtype=torch.long)\n",
    "input_mask_test = torch.tensor([features[1] for features in X_test_tokens.values], dtype=torch.long)\n",
    "label_ids_test = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = TensorDataset(input_ids_train, input_mask_train, label_ids_train)\n",
    "test_dataset = TensorDataset(input_ids_test, input_mask_test, label_ids_test)\n",
    "\n",
    "# 训练参数\n",
    "train_batch_size = 64\n",
    "num_train_epochs = 3\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
    "t_total = len(train_dataloader) // num_train_epochs\n",
    "\n",
    "# 优化器和学习率调度器\n",
    "learning_rate = 5e-5\n",
    "adam_epsilon = 1e-8\n",
    "warmup_steps = 0\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "# 设备\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=torch.device('cpu')\n",
    "\n",
    "# 训练模型\n",
    "model.train()\n",
    "train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        model.zero_grad()\n",
    "        model.to(device)\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        loss = model(**inputs)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained('My_Model/weibo-bert-bilstm-crf-model')\n",
    "\n",
    "# 评估模型\n",
    "test_batch_size = 64\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)\n",
    "\n",
    "model.eval()\n",
    "preds = None\n",
    "out_label_ids = None\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"评估中\"):\n",
    "    model.to(device)\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        if preds is None:\n",
    "            preds = outputs\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, outputs, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = np.argmax(preds, axis=1)\n",
    "acc_score = accuracy_score(preds, out_label_ids)\n",
    "f1_score = f1_score(preds, out_label_ids)\n",
    "print('测试集中的Accuracy分数: ', acc_score)\n",
    "print('测试集中的F1分数: ', f1_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             mid        lat         lng  user_name  \\\n",
      "0   4.444478e+15  31.185350  121.434467      妖洞洞柒柒   \n",
      "1   4.444478e+15  31.408190  121.252110   牛哄哄-0902   \n",
      "2   4.444478e+15  31.239872  121.309017      空想家拥有   \n",
      "3   4.444478e+15  31.274380  121.467290    BeingIt   \n",
      "4   4.444478e+15  31.210000  121.561800       燕南畿东   \n",
      "..           ...        ...         ...        ...   \n",
      "95  4.444488e+15  31.128221  121.447719    隔着机窗看明月   \n",
      "96  4.444488e+15  31.229090  121.458860        青子呐   \n",
      "97  4.444488e+15  31.203160  121.474480  混在郊区的围观群众   \n",
      "98  4.444488e+15  31.317290  121.454300       明渡政七   \n",
      "99  4.444488e+15  31.189467  121.703424    苏苏的清浅时光   \n",
      "\n",
      "                         user_link verify_typ  \\\n",
      "0   https://weibo.com/u/1890269977       没有认证   \n",
      "1   https://weibo.com/u/6589039527       没有认证   \n",
      "2   https://weibo.com/u/3799023812       没有认证   \n",
      "3   https://weibo.com/u/1733443685       黄V认证   \n",
      "4      https://weibo.com/dspxiaoyu       没有认证   \n",
      "..                             ...        ...   \n",
      "95  https://weibo.com/u/1201741684       没有认证   \n",
      "96  https://weibo.com/u/5522012255       没有认证   \n",
      "97    https://weibo.com/loveyou023       没有认证   \n",
      "98  https://weibo.com/u/7051423599       没有认证   \n",
      "99  https://weibo.com/u/5384909453       没有认证   \n",
      "\n",
      "                                weibo_link      publish_time  \\\n",
      "0   https://weibo.com/1890269977/IiF5a1wgu  2019-12-01 00:00   \n",
      "1   https://weibo.com/6589039527/IiF4Mnq5n  2019-12-01 00:00   \n",
      "2   https://weibo.com/3799023812/IiF4ObSgA  2019-12-01 00:00   \n",
      "3   https://weibo.com/1733443685/IiF54oJ9e  2019-12-01 00:00   \n",
      "4   https://weibo.com/1071459682/IiF4Vy7N3  2019-12-01 00:00   \n",
      "..                                     ...               ...   \n",
      "95  https://weibo.com/1201741684/IiFkO8vhz  2019-12-01 00:39   \n",
      "96  https://weibo.com/5522012255/IiFkZnj9Z  2019-12-01 00:39   \n",
      "97  https://weibo.com/1748773307/IiFkHjNG8  2019-12-01 00:39   \n",
      "98  https://weibo.com/7051423599/IiFlc5XnU  2019-12-01 00:40   \n",
      "99  https://weibo.com/5384909453/IiFlyveKj  2019-12-01 00:41   \n",
      "\n",
      "                                              content  \\\n",
      "0     欢庆下周挺进病房🙃希望这不是最后的狂欢🤓🤓 2上海·胡桃里音乐酒馆(南洋1931店) ​​​​   \n",
      "1                           💙 💙 💙 2上海·上海工艺美术职业学院 ​​​​   \n",
      "2                        顶天立地今天的舞蹈秀太赞了👍 2上海·水岸秀苑 ​​​​   \n",
      "3                             #了不起的我# 2上海·大上海紫金城 ​​​​   \n",
      "4                      终于可以回去睡觉了 2上海·上海卓美亚喜玛拉雅酒店 ​​​​   \n",
      "..                                                ...   \n",
      "95  下午和小季吃饭到三点多，十年不见，一下子聊了三个小时，都挺好，携妻带子从哈尔滨过来上海，只为...   \n",
      "96  如果你人生至今无论聚散都没跟恋人撕破过脸反目成仇互相伤害，就已经是莫大的幸运了。 2上海·吴...   \n",
      "97                            不逛不舒服斯基 2上海·星光摄影器材城鲁班路店   \n",
      "98                             vans橘子汽水 2上海·平保小区 ​​​​   \n",
      "99   那时候真以为自己的肾怎么了，忍着绞痛到了医院，一个人忍着绞痛挂号做CT吊盐水将近三个小时，...   \n",
      "\n",
      "                                           image_urls  ... forward_num  \\\n",
      "0   https://wx4.sinaimg.cn/large/70ab3b19gy1g9gicq...  ...         0.0   \n",
      "1   https://wx3.sinaimg.cn/large/007bUWb5ly1g9gicr...  ...         0.0   \n",
      "2   https://wx3.sinaimg.cn/large/e27080c4ly1g9giah...  ...         0.0   \n",
      "3   https://wx4.sinaimg.cn/large/67524065gy1g9gidv...  ...         0.0   \n",
      "4   https://wx4.sinaimg.cn/large/3fdd2d62ly1g9gicu...  ...         0.0   \n",
      "..                                                ...  ...         ...   \n",
      "95                                                NaN  ...         0.0   \n",
      "96  https://wx2.sinaimg.cn/large/0061HNDNgy1g9gjim...  ...         0.0   \n",
      "97  https://wx2.sinaimg.cn/large/683c29bbly1g9gjgz...  ...         0.0   \n",
      "98  https://wx2.sinaimg.cn/large/007Hd3oHly1g9gjhx...  ...         0.0   \n",
      "99  https://wx4.sinaimg.cn/large/005SqwWhly1g9gjbr...  ...         0.0   \n",
      "\n",
      "    comment_num  like_num      phone_type content_location_name  \\\n",
      "0           0.0       0.0       iPhone客户端   上海·胡桃里音乐酒馆(南洋1931店)   \n",
      "1           2.0       7.0   AI实力派 vivo Z1         上海·上海工艺美术职业学院   \n",
      "2           0.0       1.0     前置双摄vivo X9               上海·水岸秀苑   \n",
      "3           0.0       1.0  HUAWEI P30 Pro             上海·大上海紫金城   \n",
      "4           0.0       1.0         Android        上海·上海卓美亚喜玛拉雅酒店   \n",
      "..          ...       ...             ...                   ...   \n",
      "95          0.0       0.0   HUAWEI Mate 9               上海·馨宁公寓   \n",
      "96          0.0       0.0       iPhone客户端             上海·吴江路休闲街   \n",
      "97          0.0       0.0  HUAWEI P20 Pro        上海·星光摄影器材城鲁班路店   \n",
      "98         15.0      21.0       小米8周年旗舰手机               上海·平保小区   \n",
      "99          2.0       0.0         荣耀10青春版        上海·上海市浦东新区人民医院   \n",
      "\n",
      "                              content_location_url     wgs_lng    wgs_lat  \\\n",
      "0   https://weibo.com/p/100101B2094551D46AABFB4593  121.429847  31.187230   \n",
      "1   https://weibo.com/p/100101B2094757D06FA6FC459D  121.247669  31.410147   \n",
      "2   https://weibo.com/p/100101B2094757D068A6FA429D  121.304516  31.241830   \n",
      "3   https://weibo.com/p/100101B2094655D26CA0FC439F  121.462743  31.276286   \n",
      "4   https://weibo.com/p/100101B2094757D06EAAFB489B  121.557560  31.212184   \n",
      "..                                             ...         ...        ...   \n",
      "95  https://weibo.com/p/100101B2094757D065A0FA4999  121.443131  31.130153   \n",
      "96  https://weibo.com/p/100101B2094757D06EAAFA4799  121.454292  31.230995   \n",
      "97  https://weibo.com/p/100101B2094655D26AA0FC4793  121.469962  31.205117   \n",
      "98  https://weibo.com/p/100101B2094654D56DA2FA4393  121.449712  31.319148   \n",
      "99  https://weibo.com/p/100101B2094757D06FA2F8449A  121.699081  31.191541   \n",
      "\n",
      "    label                                               text  \n",
      "0       0                          欢庆下周挺进病房🙃希望这不是最后的狂欢🤓🤓​​​​  \n",
      "1       0                                            💙💙💙​​​​  \n",
      "2       0                                 顶天立地今天的舞蹈秀太赞了👍​​​​  \n",
      "3       0                                          了不起的我​​​​  \n",
      "4       0                                      终于可以回去睡觉了​​​​  \n",
      "..    ...                                                ...  \n",
      "95      0  下午和小季吃饭到三点多十年不见一下子聊了三个小时都挺好携妻带子从哈尔滨过来上海只为升职更高的...  \n",
      "96      0         如果你人生至今无论聚散都没跟恋人撕破过脸反目成仇互相伤害就已经是莫大的幸运了​​​​  \n",
      "97      0                                            不逛不舒服斯基  \n",
      "98      0                                           橘子汽水​​​​  \n",
      "99      0  那时候真以为自己的肾怎么了忍着绞痛到了医院一个人忍着绞痛挂号做吊盐水将近三个小时最后还是疼得...  \n",
      "\n",
      "[98 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\ProgramData\\MachineLearning\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Predict: 100%|██████████| 1/1 [00:16<00:00, 16.95s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取数据\n",
    "df_origin = pd.read_csv(r'D:/Code/Python/senetiment/data/上海2019_2023年按月分类文件/20191201_20200101 57356 条.csv')\n",
    "# 只取三行数据\n",
    "df_origin = df_origin.head(100)\n",
    "df_origin['label'] = 0  # 统一初始化为0\n",
    "\n",
    "# 对文本进行清洗\n",
    "df_origin['text'] = df_origin.content.str.replace('\\n', ' ')\n",
    "\n",
    "def clean_content(content, place):\n",
    "    import re\n",
    "    # 删除地名\n",
    "    content = content.replace(place, '')\n",
    "    # 删除特殊字符\n",
    "    content = re.sub(r'分享图片|分享视频|微博视频|的微博视频|#|，|,|。|；|、|！|\\+|=|-|&|:|%|\\?', '', content)\n",
    "    content = re.sub(r'[：\"\\'/|……]', '', content)\n",
    "    # 去除数字和空格\n",
    "    content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.', '')\n",
    "    # 去除一些符号\n",
    "    content = re.sub(r'[_\\[\\]【】<>《》（）\\(\\)]', '', content)\n",
    "    # 去掉两个#及中间的字符\n",
    "    # content = re.sub(r'#.*#', '', content)\n",
    "    # 把a到z的字母去掉\n",
    "    content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "    # 只保留中文\n",
    "    # content = ''.join(filter(lambda x: '\\u4e00' <= x <= '\\u9fa5', content))\n",
    "    return content\n",
    "\n",
    "# 对content列进行清洗 由对应的地名在文本中去除content文本中的内容\n",
    "df_origin['text'] = df_origin.apply(lambda row: clean_content(row['text'], row['content_location_name']), axis=1)\n",
    "\n",
    "# 如果文本为空 去掉该行\n",
    "df_origin = df_origin[df_origin['text'] != '']\n",
    "\n",
    "# 保留text文本长度大于4的行\n",
    "df_origin = df_origin[df_origin['text'].str.len() > 4]\n",
    "\n",
    "\n",
    "print(df_origin)\n",
    "\n",
    "X_pred = df_origin['text']\n",
    "Y_pred = df_origin['label']\n",
    "\n",
    "# 加载BERT模型和分词器\n",
    "model_name = 'bert-base-chinese'\n",
    "# tokenizer = BertTokenizer.from_pretrained('My_Model/weibo-bert-rubbish-model')\n",
    "# model = BertForSequenceClassification.from_pretrained('My_Model/weibo-bert-rubbish-model')\n",
    "model = BertForSequenceClassification.from_pretrained('model/' + model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained('model/' + model_name)\n",
    "\n",
    "# 定义分词函数\n",
    "def get_tokens(text, tokenizer, max_seq_length, add_special_tokens=True):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=add_special_tokens, truncation=True, max_length=max_seq_length, pad_to_max_length=True)\n",
    "    attention_mask = [int(id > 0) for id in input_ids]\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(attention_mask) == max_seq_length\n",
    "    return (input_ids, attention_mask)\n",
    "\n",
    "X_pred_tokens = X_pred.apply(get_tokens, args=(tokenizer, 150))\n",
    "\n",
    "input_ids_pred = torch.tensor([features[0] for features in X_pred_tokens.values], dtype=torch.long)\n",
    "input_mask_pred = torch.tensor([features[1] for features in X_pred_tokens.values], dtype=torch.long)\n",
    "label_pred = torch.tensor(Y_pred.values, dtype=torch.long)\n",
    "pred_dataset = TensorDataset(input_ids_pred, input_mask_pred, label_pred)\n",
    "\n",
    "pred_batch_size = 256\n",
    "pred_sampler = SequentialSampler(pred_dataset)\n",
    "pred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size=pred_batch_size)\n",
    "\n",
    "# 预测\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "preds = None\n",
    "for batch in tqdm(pred_dataloader, desc=\"Predict\"):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        _, logits = outputs[:2]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "prob = torch.nn.functional.softmax(torch.tensor(preds), dim=1)  # 使用softmax函数计算预测的概率分布\n",
    "preds = np.argmax(preds, axis=1)  # 计算每个样本的最终预测类别\n",
    "df_origin['ad_prob'] = [p[1].item() for p in prob]  # 将概率分布的第二列（表示\"1\"类别的概率）添加到DataFrame中\n",
    "df_origin['pred'] = preds  # 将最终的预测类别添加到DataFrame中\n",
    "\n",
    "# print(df_origin.sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin.to_csv('sentiment_analysis_data.csv', index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras下的LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Embedding,Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load dataset\n",
    "# ['evaluation'] is feature, ['label'] is label\n",
    "def load_data(filepath,input_shape=20):\n",
    "    df=pd.read_csv(filepath,encoding='gbk')\n",
    "\n",
    "    # 标签及词汇表\n",
    "    labels,vocabulary=list(df['label'].unique()),list(df['evaluation'].unique())\n",
    "\n",
    "    # 构造字符级别的特征\n",
    "    string=''\n",
    "    for word in vocabulary:\n",
    "        string+=word\n",
    "\n",
    "    vocabulary=set(string)\n",
    "\n",
    "    # 字典列表\n",
    "    word_dictionary={word:i+1 for i,word in enumerate(vocabulary)}\n",
    "    with open('word_dict.pk','wb') as f:\n",
    "        pickle.dump(word_dictionary,f)\n",
    "    inverse_word_dictionary={i+1:word for i,word in enumerate(vocabulary)}\n",
    "    label_dictionary={label:i for i,label in enumerate(labels)}\n",
    "    with open('label_dict.pk','wb') as f:\n",
    "        pickle.dump(label_dictionary,f)\n",
    "    output_dictionary={i:labels for i,labels in enumerate(labels)}\n",
    "\n",
    "    # 词汇表大小\n",
    "    vocab_size=len(word_dictionary.keys())\n",
    "    # 标签类别数量\n",
    "    label_size=len(label_dictionary.keys())\n",
    "\n",
    "    # 序列填充，按input_shape填充，长度不足的按0补充\n",
    "    x=[[word_dictionary[word] for word in sent] for sent in df['evaluation']]\n",
    "    x=pad_sequences(maxlen=input_shape,sequences=x,padding='post',value=0)\n",
    "    y=[[label_dictionary[sent]] for sent in df['label']]\n",
    "    '''\n",
    "    np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)\n",
    "    的二值序列。\n",
    "    假设num_classes = 10。\n",
    "    如将[1, 2, 3,……4]转化成：\n",
    "    [[0, 1, 0, 0, 0, 0, 0, 0]\n",
    "     [0, 0, 1, 0, 0, 0, 0, 0]\n",
    "     [0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    ……\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0]]\n",
    "    '''\n",
    "    y=[np_utils.to_categorical(label,num_classes=label_size) for label in y]\n",
    "    y=np.array([list(_[0]) for _ in y])\n",
    "\n",
    "    return x,y,output_dictionary,vocab_size,label_size,inverse_word_dictionary\n",
    "\n",
    "# 创建深度学习模型，Embedding + LSTM + Softmax\n",
    "def create_LSTM(n_units,input_shape,output_dim,filepath):\n",
    "    x,y,output_dictionary,vocab_size,label_size,inverse_word_dictionary=load_data(filepath)\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size+1,output_dim=output_dim,\n",
    "                        input_length=input_shape,mask_zero=True))\n",
    "    model.add(LSTM(n_units,input_shape=(x.shape[0],x.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(label_size,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    plot_model(model,to_file='./model_lstm.png',show_shapes=True)\n",
    "    # 输出模型信息\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# 模型训练\n",
    "def model_train(input_shape,filepath,model_save_path):\n",
    "    # 将数据集分为训练集和测试集，占比为9：1\n",
    "    # input_shape=100\n",
    "    x,y,output_dictionary,vocab_size,label_size,inverse_word_dictionary=load_data(filepath,input_shape)\n",
    "    train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.1,random_state=42)\n",
    "\n",
    "    # 模型输入参数，需要根据自己需要调整\n",
    "    n_units=100\n",
    "    batch_size=32\n",
    "    epochs=5\n",
    "    output_dim=20\n",
    "\n",
    "    # 模型训练\n",
    "    lstm_model=create_LSTM(n_units,input_shape,output_dim,filepath)\n",
    "    lstm_model.fit(train_x,train_y,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "\n",
    "    # 模型保存\n",
    "    lstm_model.save(model_save_path)\n",
    "\n",
    "    # 测试条数\n",
    "    N= test_x.shape[0]\n",
    "    predict=[]\n",
    "    label=[]\n",
    "    for start,end in zip(range(0,N,1),range(1,N+1,1)):\n",
    "        print(f'start:{start}, end:{end}')\n",
    "        sentence=[inverse_word_dictionary[i] for i in test_x[start] if i!=0]\n",
    "        y_predict=lstm_model.predict(test_x[start:end])\n",
    "        print('y_predict:',y_predict)\n",
    "        label_predict=output_dictionary[np.argmax(y_predict[0])]\n",
    "        label_true=output_dictionary[np.argmax(test_y[start:end])]\n",
    "        print(f'label_predict:{label_predict}, label_true:{label_true}')\n",
    "        # 输出预测结果\n",
    "        print(''.join(sentence),label_true,label_predict)\n",
    "        predict.append(label_predict)\n",
    "        label.append(label_true)\n",
    "\n",
    "    # 预测准确率\n",
    "    acc=accuracy_score(predict,label)\n",
    "    print('模型在测试集上的准确率:%s'%acc)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    filepath='C:\\\\数据集\\\\情感分析60000\\\\all.csv'\n",
    "    input_shape=180\n",
    "    model_save_path='C:\\\\数据集\\\\情感分析60000\\\\corpus_model.h5'\n",
    "    model_train(input_shape,filepath,model_save_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

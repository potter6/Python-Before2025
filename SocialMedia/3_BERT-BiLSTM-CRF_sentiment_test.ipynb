{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-BiLSTM-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\ProgramData\\anaconda3\\envs\\GIS\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\anaconda3\\envs\\GIS\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\anaconda3\\envs\\GIS\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31måœ¨å½“å‰å•å…ƒæ ¼æˆ–ä¸Šä¸€ä¸ªå•å…ƒæ ¼ä¸­æ‰§è¡Œä»£ç æ—¶ Kernel å´©æºƒã€‚\n",
      "\u001b[1;31mè¯·æŸ¥çœ‹å•å…ƒæ ¼ä¸­çš„ä»£ç ï¼Œä»¥ç¡®å®šæ•…éšœçš„å¯èƒ½åŸå› ã€‚\n",
      "\u001b[1;31må•å‡»<a href='https://aka.ms/vscodeJupyterKernelCrash'>æ­¤å¤„</a>äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚\n",
      "\u001b[1;31mæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ Jupyter <a href='command:jupyter.viewOutput'>log</a>ã€‚"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "from pandarallel import pandarallel\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import trange, tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchcrf import CRF\n",
    "from torch.nn import LSTM\n",
    "\n",
    "# åˆå§‹åŒ–pandarallel\n",
    "pandarallel.initialize(nb_workers=psutil.cpu_count(logical=False))\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df_label = pd.read_csv('data\\WeiboTrainData\\weibo_label.csv')\n",
    "\n",
    "# å®šä¹‰æ•°æ®æ¸…æ´—å‡½æ•°\n",
    "def clean_content(content):\n",
    "    import re\n",
    "    content = re.sub(r'åˆ†äº«å›¾ç‰‡|åˆ†äº«è§†é¢‘|å¾®åšè§†é¢‘|çš„å¾®åšè§†é¢‘|#|ï¼Œ|,|ã€‚|ï¼›|ã€|ï¼|\\+|=|-|&|:|%|\\?', '', content)\n",
    "    content = re.sub(r'[ï¼š\"\\'/|â€¦â€¦]', '', content)\n",
    "    content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.', '')\n",
    "    content = re.sub(r'[_î˜§\\[\\]ã€ã€‘<>ã€Šã€‹ï¼ˆï¼‰\\(\\)]', '', content)\n",
    "    content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "    return content\n",
    "\n",
    "# æ¸…æ´—æ•°æ®\n",
    "df_label['message'] = df_label['message'].parallel_apply(clean_content)\n",
    "\n",
    "# å»é™¤ç©ºçš„è¡Œ\n",
    "df_label= df_label[df_label['message'] != '']\n",
    "\n",
    "# ä¿ç•™textæ–‡æœ¬é•¿åº¦å¤§äº4çš„è¡Œ\n",
    "df_label = df_label[df_label['message'].str.len() > 4]\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "df_label.sentiment.value_counts()\n",
    "df = df_label.copy()\n",
    "df.loc[df[df.sentiment!=6].index,'sentiment'] = 1\n",
    "df = df[(df['sentiment']==1) | (df['sentiment']==6)]\n",
    "drop_size = len(df[df['sentiment']==1].sentiment) - len(df[df['sentiment']==6].sentiment)\n",
    "df.drop(df[df['sentiment']==1].sample(drop_size).index, inplace=True)\n",
    "df.loc[df[df.sentiment==6].index,'sentiment'] = 0\n",
    "df.sentiment.unique()\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['message'], df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# åŠ è½½BERTæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "model_name = 'bert-base-chinese'\n",
    "config = BertConfig.from_pretrained('model/' + model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained('model/' + model_name)\n",
    "bert_model = BertForSequenceClassification.from_pretrained('model/' + model_name, num_labels=2)\n",
    "# bert_model = BertForSequenceClassification.from_pretrained('model/' + model_name, num_labels=768)\n",
    "\n",
    "# å®šä¹‰BERT-BiLSTM-CRFæ¨¡å‹\n",
    "class BertBiLSTMCRF(torch.nn.Module):                                      \n",
    "    def __init__(self, bert_model, num_labels, hidden_dim):\n",
    "        super(BertBiLSTMCRF, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.lstm = LSTM(input_size=bert_model.config.hidden_size, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim * 2, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        lstm_output, _ = self.lstm(sequence_output)\n",
    "        emissions = self.fc(lstm_output)\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(emissions, labels, mask=attention_mask.byte())\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(emissions, mask=attention_mask.byte())\n",
    "\n",
    "# model = BertBiLSTMCRF(bert_model, num_labels=2, hidden_dim=256)\n",
    "model = BertBiLSTMCRF(bert_model, num_labels=3, hidden_dim=256)\n",
    "\n",
    "# å®šä¹‰åˆ†è¯å‡½æ•°\n",
    "def get_tokens(text, tokenizer, max_seq_length, add_special_tokens=True):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=add_special_tokens, truncation=True, max_length=max_seq_length, pad_to_max_length=True)\n",
    "    attention_mask = [int(id > 0) for id in input_ids]\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(attention_mask) == max_seq_length\n",
    "    return (input_ids, attention_mask)\n",
    "\n",
    "# å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œåˆ†è¯\n",
    "X_train_tokens = X_train.apply(get_tokens, args=(tokenizer, 150))\n",
    "X_test_tokens = X_test.apply(get_tokens, args=(tokenizer, 150))\n",
    "\n",
    "# è½¬æ¢ä¸ºPyTorchå¼ é‡\n",
    "input_ids_train = torch.tensor([features[0] for features in X_train_tokens.values], dtype=torch.long)\n",
    "input_mask_train = torch.tensor([features[1] for features in X_train_tokens.values], dtype=torch.long)\n",
    "label_ids_train = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "\n",
    "input_ids_test = torch.tensor([features[0] for features in X_test_tokens.values], dtype=torch.long)\n",
    "input_mask_test = torch.tensor([features[1] for features in X_test_tokens.values], dtype=torch.long)\n",
    "label_ids_test = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†\n",
    "train_dataset = TensorDataset(input_ids_train, input_mask_train, label_ids_train)\n",
    "test_dataset = TensorDataset(input_ids_test, input_mask_test, label_ids_test)\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "train_batch_size = 64\n",
    "num_train_epochs = 3\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
    "t_total = len(train_dataloader) // num_train_epochs\n",
    "\n",
    "# ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "learning_rate = 5e-5\n",
    "adam_epsilon = 1e-8\n",
    "warmup_steps = 0\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "# è®¾å¤‡\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=torch.device('cpu')\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model.train()\n",
    "train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        model.zero_grad()\n",
    "        model.to(device)\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        loss = model(**inputs)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "model.save_pretrained('My_Model/weibo-bert-bilstm-crf-model')\n",
    "\n",
    "# è¯„ä¼°æ¨¡å‹\n",
    "test_batch_size = 64\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)\n",
    "\n",
    "model.eval()\n",
    "preds = None\n",
    "out_label_ids = None\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"è¯„ä¼°ä¸­\"):\n",
    "    model.to(device)\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        if preds is None:\n",
    "            preds = outputs\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, outputs, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = np.argmax(preds, axis=1)\n",
    "acc_score = accuracy_score(preds, out_label_ids)\n",
    "f1_score = f1_score(preds, out_label_ids)\n",
    "print('æµ‹è¯•é›†ä¸­çš„Accuracyåˆ†æ•°: ', acc_score)\n",
    "print('æµ‹è¯•é›†ä¸­çš„F1åˆ†æ•°: ', f1_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             mid        lat         lng  user_name  \\\n",
      "0   4.444478e+15  31.185350  121.434467      å¦–æ´æ´æŸ’æŸ’   \n",
      "1   4.444478e+15  31.408190  121.252110   ç‰›å“„å“„-0902   \n",
      "2   4.444478e+15  31.239872  121.309017      ç©ºæƒ³å®¶æ‹¥æœ‰   \n",
      "3   4.444478e+15  31.274380  121.467290    BeingIt   \n",
      "4   4.444478e+15  31.210000  121.561800       ç‡•å—ç•¿ä¸œ   \n",
      "..           ...        ...         ...        ...   \n",
      "95  4.444488e+15  31.128221  121.447719    éš”ç€æœºçª—çœ‹æ˜æœˆ   \n",
      "96  4.444488e+15  31.229090  121.458860        é’å­å‘   \n",
      "97  4.444488e+15  31.203160  121.474480  æ··åœ¨éƒŠåŒºçš„å›´è§‚ç¾¤ä¼—   \n",
      "98  4.444488e+15  31.317290  121.454300       æ˜æ¸¡æ”¿ä¸ƒ   \n",
      "99  4.444488e+15  31.189467  121.703424    è‹è‹çš„æ¸…æµ…æ—¶å…‰   \n",
      "\n",
      "                         user_link verify_typ  \\\n",
      "0   https://weibo.com/u/1890269977       æ²¡æœ‰è®¤è¯   \n",
      "1   https://weibo.com/u/6589039527       æ²¡æœ‰è®¤è¯   \n",
      "2   https://weibo.com/u/3799023812       æ²¡æœ‰è®¤è¯   \n",
      "3   https://weibo.com/u/1733443685       é»„Vè®¤è¯   \n",
      "4      https://weibo.com/dspxiaoyu       æ²¡æœ‰è®¤è¯   \n",
      "..                             ...        ...   \n",
      "95  https://weibo.com/u/1201741684       æ²¡æœ‰è®¤è¯   \n",
      "96  https://weibo.com/u/5522012255       æ²¡æœ‰è®¤è¯   \n",
      "97    https://weibo.com/loveyou023       æ²¡æœ‰è®¤è¯   \n",
      "98  https://weibo.com/u/7051423599       æ²¡æœ‰è®¤è¯   \n",
      "99  https://weibo.com/u/5384909453       æ²¡æœ‰è®¤è¯   \n",
      "\n",
      "                                weibo_link      publish_time  \\\n",
      "0   https://weibo.com/1890269977/IiF5a1wgu  2019-12-01 00:00   \n",
      "1   https://weibo.com/6589039527/IiF4Mnq5n  2019-12-01 00:00   \n",
      "2   https://weibo.com/3799023812/IiF4ObSgA  2019-12-01 00:00   \n",
      "3   https://weibo.com/1733443685/IiF54oJ9e  2019-12-01 00:00   \n",
      "4   https://weibo.com/1071459682/IiF4Vy7N3  2019-12-01 00:00   \n",
      "..                                     ...               ...   \n",
      "95  https://weibo.com/1201741684/IiFkO8vhz  2019-12-01 00:39   \n",
      "96  https://weibo.com/5522012255/IiFkZnj9Z  2019-12-01 00:39   \n",
      "97  https://weibo.com/1748773307/IiFkHjNG8  2019-12-01 00:39   \n",
      "98  https://weibo.com/7051423599/IiFlc5XnU  2019-12-01 00:40   \n",
      "99  https://weibo.com/5384909453/IiFlyveKj  2019-12-01 00:41   \n",
      "\n",
      "                                              content  \\\n",
      "0     æ¬¢åº†ä¸‹å‘¨æŒºè¿›ç—…æˆ¿ğŸ™ƒå¸Œæœ›è¿™ä¸æ˜¯æœ€åçš„ç‹‚æ¬¢ğŸ¤“ğŸ¤“ 2ä¸Šæµ·Â·èƒ¡æ¡ƒé‡ŒéŸ³ä¹é…’é¦†(å—æ´‹1931åº—) â€‹â€‹â€‹â€‹   \n",
      "1                           ğŸ’™ ğŸ’™ ğŸ’™ 2ä¸Šæµ·Â·ä¸Šæµ·å·¥è‰ºç¾æœ¯èŒä¸šå­¦é™¢ â€‹â€‹â€‹â€‹   \n",
      "2                        é¡¶å¤©ç«‹åœ°ä»Šå¤©çš„èˆè¹ˆç§€å¤ªèµäº†ğŸ‘ 2ä¸Šæµ·Â·æ°´å²¸ç§€è‹‘ â€‹â€‹â€‹â€‹   \n",
      "3                             #äº†ä¸èµ·çš„æˆ‘# 2ä¸Šæµ·Â·å¤§ä¸Šæµ·ç´«é‡‘åŸ â€‹â€‹â€‹â€‹   \n",
      "4                      ç»ˆäºå¯ä»¥å›å»ç¡è§‰äº† 2ä¸Šæµ·Â·ä¸Šæµ·å“ç¾äºšå–œç›æ‹‰é›…é…’åº— â€‹â€‹â€‹â€‹   \n",
      "..                                                ...   \n",
      "95  ä¸‹åˆå’Œå°å­£åƒé¥­åˆ°ä¸‰ç‚¹å¤šï¼Œåå¹´ä¸è§ï¼Œä¸€ä¸‹å­èŠäº†ä¸‰ä¸ªå°æ—¶ï¼Œéƒ½æŒºå¥½ï¼Œæºå¦»å¸¦å­ä»å“ˆå°”æ»¨è¿‡æ¥ä¸Šæµ·ï¼Œåªä¸º...   \n",
      "96  å¦‚æœä½ äººç”Ÿè‡³ä»Šæ— è®ºèšæ•£éƒ½æ²¡è·Ÿæ‹äººæ’•ç ´è¿‡è„¸åç›®æˆä»‡äº’ç›¸ä¼¤å®³ï¼Œå°±å·²ç»æ˜¯è«å¤§çš„å¹¸è¿äº†ã€‚ 2ä¸Šæµ·Â·å´...   \n",
      "97                            ä¸é€›ä¸èˆ’æœæ–¯åŸº 2ä¸Šæµ·Â·æ˜Ÿå…‰æ‘„å½±å™¨æåŸé²ç­è·¯åº—   \n",
      "98                             vansæ©˜å­æ±½æ°´ 2ä¸Šæµ·Â·å¹³ä¿å°åŒº â€‹â€‹â€‹â€‹   \n",
      "99   é‚£æ—¶å€™çœŸä»¥ä¸ºè‡ªå·±çš„è‚¾æ€ä¹ˆäº†ï¼Œå¿ç€ç»ç—›åˆ°äº†åŒ»é™¢ï¼Œä¸€ä¸ªäººå¿ç€ç»ç—›æŒ‚å·åšCTåŠç›æ°´å°†è¿‘ä¸‰ä¸ªå°æ—¶ï¼Œ...   \n",
      "\n",
      "                                           image_urls  ... forward_num  \\\n",
      "0   https://wx4.sinaimg.cn/large/70ab3b19gy1g9gicq...  ...         0.0   \n",
      "1   https://wx3.sinaimg.cn/large/007bUWb5ly1g9gicr...  ...         0.0   \n",
      "2   https://wx3.sinaimg.cn/large/e27080c4ly1g9giah...  ...         0.0   \n",
      "3   https://wx4.sinaimg.cn/large/67524065gy1g9gidv...  ...         0.0   \n",
      "4   https://wx4.sinaimg.cn/large/3fdd2d62ly1g9gicu...  ...         0.0   \n",
      "..                                                ...  ...         ...   \n",
      "95                                                NaN  ...         0.0   \n",
      "96  https://wx2.sinaimg.cn/large/0061HNDNgy1g9gjim...  ...         0.0   \n",
      "97  https://wx2.sinaimg.cn/large/683c29bbly1g9gjgz...  ...         0.0   \n",
      "98  https://wx2.sinaimg.cn/large/007Hd3oHly1g9gjhx...  ...         0.0   \n",
      "99  https://wx4.sinaimg.cn/large/005SqwWhly1g9gjbr...  ...         0.0   \n",
      "\n",
      "    comment_num  like_num      phone_type content_location_name  \\\n",
      "0           0.0       0.0       iPhoneå®¢æˆ·ç«¯   ä¸Šæµ·Â·èƒ¡æ¡ƒé‡ŒéŸ³ä¹é…’é¦†(å—æ´‹1931åº—)   \n",
      "1           2.0       7.0   AIå®åŠ›æ´¾ vivo Z1         ä¸Šæµ·Â·ä¸Šæµ·å·¥è‰ºç¾æœ¯èŒä¸šå­¦é™¢   \n",
      "2           0.0       1.0     å‰ç½®åŒæ‘„vivo X9               ä¸Šæµ·Â·æ°´å²¸ç§€è‹‘   \n",
      "3           0.0       1.0  HUAWEI P30 Pro             ä¸Šæµ·Â·å¤§ä¸Šæµ·ç´«é‡‘åŸ   \n",
      "4           0.0       1.0         Android        ä¸Šæµ·Â·ä¸Šæµ·å“ç¾äºšå–œç›æ‹‰é›…é…’åº—   \n",
      "..          ...       ...             ...                   ...   \n",
      "95          0.0       0.0   HUAWEI Mate 9               ä¸Šæµ·Â·é¦¨å®å…¬å¯“   \n",
      "96          0.0       0.0       iPhoneå®¢æˆ·ç«¯             ä¸Šæµ·Â·å´æ±Ÿè·¯ä¼‘é—²è¡—   \n",
      "97          0.0       0.0  HUAWEI P20 Pro        ä¸Šæµ·Â·æ˜Ÿå…‰æ‘„å½±å™¨æåŸé²ç­è·¯åº—   \n",
      "98         15.0      21.0       å°ç±³8å‘¨å¹´æ——èˆ°æ‰‹æœº               ä¸Šæµ·Â·å¹³ä¿å°åŒº   \n",
      "99          2.0       0.0         è£è€€10é’æ˜¥ç‰ˆ        ä¸Šæµ·Â·ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒºäººæ°‘åŒ»é™¢   \n",
      "\n",
      "                              content_location_url     wgs_lng    wgs_lat  \\\n",
      "0   https://weibo.com/p/100101B2094551D46AABFB4593  121.429847  31.187230   \n",
      "1   https://weibo.com/p/100101B2094757D06FA6FC459D  121.247669  31.410147   \n",
      "2   https://weibo.com/p/100101B2094757D068A6FA429D  121.304516  31.241830   \n",
      "3   https://weibo.com/p/100101B2094655D26CA0FC439F  121.462743  31.276286   \n",
      "4   https://weibo.com/p/100101B2094757D06EAAFB489B  121.557560  31.212184   \n",
      "..                                             ...         ...        ...   \n",
      "95  https://weibo.com/p/100101B2094757D065A0FA4999  121.443131  31.130153   \n",
      "96  https://weibo.com/p/100101B2094757D06EAAFA4799  121.454292  31.230995   \n",
      "97  https://weibo.com/p/100101B2094655D26AA0FC4793  121.469962  31.205117   \n",
      "98  https://weibo.com/p/100101B2094654D56DA2FA4393  121.449712  31.319148   \n",
      "99  https://weibo.com/p/100101B2094757D06FA2F8449A  121.699081  31.191541   \n",
      "\n",
      "    label                                               text  \n",
      "0       0                          æ¬¢åº†ä¸‹å‘¨æŒºè¿›ç—…æˆ¿ğŸ™ƒå¸Œæœ›è¿™ä¸æ˜¯æœ€åçš„ç‹‚æ¬¢ğŸ¤“ğŸ¤“â€‹â€‹â€‹â€‹  \n",
      "1       0                                            ğŸ’™ğŸ’™ğŸ’™â€‹â€‹â€‹â€‹  \n",
      "2       0                                 é¡¶å¤©ç«‹åœ°ä»Šå¤©çš„èˆè¹ˆç§€å¤ªèµäº†ğŸ‘â€‹â€‹â€‹â€‹  \n",
      "3       0                                          äº†ä¸èµ·çš„æˆ‘â€‹â€‹â€‹â€‹  \n",
      "4       0                                      ç»ˆäºå¯ä»¥å›å»ç¡è§‰äº†â€‹â€‹â€‹â€‹  \n",
      "..    ...                                                ...  \n",
      "95      0  ä¸‹åˆå’Œå°å­£åƒé¥­åˆ°ä¸‰ç‚¹å¤šåå¹´ä¸è§ä¸€ä¸‹å­èŠäº†ä¸‰ä¸ªå°æ—¶éƒ½æŒºå¥½æºå¦»å¸¦å­ä»å“ˆå°”æ»¨è¿‡æ¥ä¸Šæµ·åªä¸ºå‡èŒæ›´é«˜çš„...  \n",
      "96      0         å¦‚æœä½ äººç”Ÿè‡³ä»Šæ— è®ºèšæ•£éƒ½æ²¡è·Ÿæ‹äººæ’•ç ´è¿‡è„¸åç›®æˆä»‡äº’ç›¸ä¼¤å®³å°±å·²ç»æ˜¯è«å¤§çš„å¹¸è¿äº†â€‹â€‹â€‹â€‹  \n",
      "97      0                                            ä¸é€›ä¸èˆ’æœæ–¯åŸº  \n",
      "98      0                                           æ©˜å­æ±½æ°´â€‹â€‹â€‹â€‹  \n",
      "99      0  é‚£æ—¶å€™çœŸä»¥ä¸ºè‡ªå·±çš„è‚¾æ€ä¹ˆäº†å¿ç€ç»ç—›åˆ°äº†åŒ»é™¢ä¸€ä¸ªäººå¿ç€ç»ç—›æŒ‚å·åšåŠç›æ°´å°†è¿‘ä¸‰ä¸ªå°æ—¶æœ€åè¿˜æ˜¯ç–¼å¾—...  \n",
      "\n",
      "[98 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\ProgramData\\MachineLearning\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:16<00:00, 16.95s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df_origin = pd.read_csv(r'D:/Code/Python/senetiment/data/ä¸Šæµ·2019_2023å¹´æŒ‰æœˆåˆ†ç±»æ–‡ä»¶/20191201_20200101 57356 æ¡.csv')\n",
    "# åªå–ä¸‰è¡Œæ•°æ®\n",
    "df_origin = df_origin.head(100)\n",
    "df_origin['label'] = 0  # ç»Ÿä¸€åˆå§‹åŒ–ä¸º0\n",
    "\n",
    "# å¯¹æ–‡æœ¬è¿›è¡Œæ¸…æ´—\n",
    "df_origin['text'] = df_origin.content.str.replace('\\n', ' ')\n",
    "\n",
    "def clean_content(content, place):\n",
    "    import re\n",
    "    # åˆ é™¤åœ°å\n",
    "    content = content.replace(place, '')\n",
    "    # åˆ é™¤ç‰¹æ®Šå­—ç¬¦\n",
    "    content = re.sub(r'åˆ†äº«å›¾ç‰‡|åˆ†äº«è§†é¢‘|å¾®åšè§†é¢‘|çš„å¾®åšè§†é¢‘|#|ï¼Œ|,|ã€‚|ï¼›|ã€|ï¼|\\+|=|-|&|:|%|\\?', '', content)\n",
    "    content = re.sub(r'[ï¼š\"\\'/|â€¦â€¦]', '', content)\n",
    "    # å»é™¤æ•°å­—å’Œç©ºæ ¼\n",
    "    content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.', '')\n",
    "    # å»é™¤ä¸€äº›ç¬¦å·\n",
    "    content = re.sub(r'[_î˜§\\[\\]ã€ã€‘<>ã€Šã€‹ï¼ˆï¼‰\\(\\)]', '', content)\n",
    "    # å»æ‰ä¸¤ä¸ª#åŠä¸­é—´çš„å­—ç¬¦\n",
    "    # content = re.sub(r'#.*#', '', content)\n",
    "    # æŠŠaåˆ°zçš„å­—æ¯å»æ‰\n",
    "    content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "    # åªä¿ç•™ä¸­æ–‡\n",
    "    # content = ''.join(filter(lambda x: '\\u4e00' <= x <= '\\u9fa5', content))\n",
    "    return content\n",
    "\n",
    "# å¯¹contentåˆ—è¿›è¡Œæ¸…æ´— ç”±å¯¹åº”çš„åœ°ååœ¨æ–‡æœ¬ä¸­å»é™¤contentæ–‡æœ¬ä¸­çš„å†…å®¹\n",
    "df_origin['text'] = df_origin.apply(lambda row: clean_content(row['text'], row['content_location_name']), axis=1)\n",
    "\n",
    "# å¦‚æœæ–‡æœ¬ä¸ºç©º å»æ‰è¯¥è¡Œ\n",
    "df_origin = df_origin[df_origin['text'] != '']\n",
    "\n",
    "# ä¿ç•™textæ–‡æœ¬é•¿åº¦å¤§äº4çš„è¡Œ\n",
    "df_origin = df_origin[df_origin['text'].str.len() > 4]\n",
    "\n",
    "\n",
    "print(df_origin)\n",
    "\n",
    "X_pred = df_origin['text']\n",
    "Y_pred = df_origin['label']\n",
    "\n",
    "# åŠ è½½BERTæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "model_name = 'bert-base-chinese'\n",
    "# tokenizer = BertTokenizer.from_pretrained('My_Model/weibo-bert-rubbish-model')\n",
    "# model = BertForSequenceClassification.from_pretrained('My_Model/weibo-bert-rubbish-model')\n",
    "model = BertForSequenceClassification.from_pretrained('model/' + model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained('model/' + model_name)\n",
    "\n",
    "# å®šä¹‰åˆ†è¯å‡½æ•°\n",
    "def get_tokens(text, tokenizer, max_seq_length, add_special_tokens=True):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=add_special_tokens, truncation=True, max_length=max_seq_length, pad_to_max_length=True)\n",
    "    attention_mask = [int(id > 0) for id in input_ids]\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(attention_mask) == max_seq_length\n",
    "    return (input_ids, attention_mask)\n",
    "\n",
    "X_pred_tokens = X_pred.apply(get_tokens, args=(tokenizer, 150))\n",
    "\n",
    "input_ids_pred = torch.tensor([features[0] for features in X_pred_tokens.values], dtype=torch.long)\n",
    "input_mask_pred = torch.tensor([features[1] for features in X_pred_tokens.values], dtype=torch.long)\n",
    "label_pred = torch.tensor(Y_pred.values, dtype=torch.long)\n",
    "pred_dataset = TensorDataset(input_ids_pred, input_mask_pred, label_pred)\n",
    "\n",
    "pred_batch_size = 256\n",
    "pred_sampler = SequentialSampler(pred_dataset)\n",
    "pred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size=pred_batch_size)\n",
    "\n",
    "# é¢„æµ‹\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "preds = None\n",
    "for batch in tqdm(pred_dataloader, desc=\"Predict\"):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        _, logits = outputs[:2]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "prob = torch.nn.functional.softmax(torch.tensor(preds), dim=1)  # ä½¿ç”¨softmaxå‡½æ•°è®¡ç®—é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒ\n",
    "preds = np.argmax(preds, axis=1)  # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æœ€ç»ˆé¢„æµ‹ç±»åˆ«\n",
    "df_origin['ad_prob'] = [p[1].item() for p in prob]  # å°†æ¦‚ç‡åˆ†å¸ƒçš„ç¬¬äºŒåˆ—ï¼ˆè¡¨ç¤º\"1\"ç±»åˆ«çš„æ¦‚ç‡ï¼‰æ·»åŠ åˆ°DataFrameä¸­\n",
    "df_origin['pred'] = preds  # å°†æœ€ç»ˆçš„é¢„æµ‹ç±»åˆ«æ·»åŠ åˆ°DataFrameä¸­\n",
    "\n",
    "# print(df_origin.sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin.to_csv('sentiment_analysis_data.csv', index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kerasä¸‹çš„LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Embedding,Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load dataset\n",
    "# ['evaluation'] is feature, ['label'] is label\n",
    "def load_data(filepath,input_shape=20):\n",
    "    df=pd.read_csv(filepath,encoding='gbk')\n",
    "\n",
    "    # æ ‡ç­¾åŠè¯æ±‡è¡¨\n",
    "    labels,vocabulary=list(df['label'].unique()),list(df['evaluation'].unique())\n",
    "\n",
    "    # æ„é€ å­—ç¬¦çº§åˆ«çš„ç‰¹å¾\n",
    "    string=''\n",
    "    for word in vocabulary:\n",
    "        string+=word\n",
    "\n",
    "    vocabulary=set(string)\n",
    "\n",
    "    # å­—å…¸åˆ—è¡¨\n",
    "    word_dictionary={word:i+1 for i,word in enumerate(vocabulary)}\n",
    "    with open('word_dict.pk','wb') as f:\n",
    "        pickle.dump(word_dictionary,f)\n",
    "    inverse_word_dictionary={i+1:word for i,word in enumerate(vocabulary)}\n",
    "    label_dictionary={label:i for i,label in enumerate(labels)}\n",
    "    with open('label_dict.pk','wb') as f:\n",
    "        pickle.dump(label_dictionary,f)\n",
    "    output_dictionary={i:labels for i,labels in enumerate(labels)}\n",
    "\n",
    "    # è¯æ±‡è¡¨å¤§å°\n",
    "    vocab_size=len(word_dictionary.keys())\n",
    "    # æ ‡ç­¾ç±»åˆ«æ•°é‡\n",
    "    label_size=len(label_dictionary.keys())\n",
    "\n",
    "    # åºåˆ—å¡«å……ï¼ŒæŒ‰input_shapeå¡«å……ï¼Œé•¿åº¦ä¸è¶³çš„æŒ‰0è¡¥å……\n",
    "    x=[[word_dictionary[word] for word in sent] for sent in df['evaluation']]\n",
    "    x=pad_sequences(maxlen=input_shape,sequences=x,padding='post',value=0)\n",
    "    y=[[label_dictionary[sent]] for sent in df['label']]\n",
    "    '''\n",
    "    np_utils.to_categoricalç”¨äºå°†æ ‡ç­¾è½¬åŒ–ä¸ºå½¢å¦‚(nb_samples, nb_classes)\n",
    "    çš„äºŒå€¼åºåˆ—ã€‚\n",
    "    å‡è®¾num_classes = 10ã€‚\n",
    "    å¦‚å°†[1, 2, 3,â€¦â€¦4]è½¬åŒ–æˆï¼š\n",
    "    [[0, 1, 0, 0, 0, 0, 0, 0]\n",
    "     [0, 0, 1, 0, 0, 0, 0, 0]\n",
    "     [0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    â€¦â€¦\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0]]\n",
    "    '''\n",
    "    y=[np_utils.to_categorical(label,num_classes=label_size) for label in y]\n",
    "    y=np.array([list(_[0]) for _ in y])\n",
    "\n",
    "    return x,y,output_dictionary,vocab_size,label_size,inverse_word_dictionary\n",
    "\n",
    "# åˆ›å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒEmbedding + LSTM + Softmax\n",
    "def create_LSTM(n_units,input_shape,output_dim,filepath):\n",
    "    x,y,output_dictionary,vocab_size,label_size,inverse_word_dictionary=load_data(filepath)\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size+1,output_dim=output_dim,\n",
    "                        input_length=input_shape,mask_zero=True))\n",
    "    model.add(LSTM(n_units,input_shape=(x.shape[0],x.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(label_size,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    plot_model(model,to_file='./model_lstm.png',show_shapes=True)\n",
    "    # è¾“å‡ºæ¨¡å‹ä¿¡æ¯\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# æ¨¡å‹è®­ç»ƒ\n",
    "def model_train(input_shape,filepath,model_save_path):\n",
    "    # å°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå æ¯”ä¸º9ï¼š1\n",
    "    # input_shape=100\n",
    "    x,y,output_dictionary,vocab_size,label_size,inverse_word_dictionary=load_data(filepath,input_shape)\n",
    "    train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.1,random_state=42)\n",
    "\n",
    "    # æ¨¡å‹è¾“å…¥å‚æ•°ï¼Œéœ€è¦æ ¹æ®è‡ªå·±éœ€è¦è°ƒæ•´\n",
    "    n_units=100\n",
    "    batch_size=32\n",
    "    epochs=5\n",
    "    output_dim=20\n",
    "\n",
    "    # æ¨¡å‹è®­ç»ƒ\n",
    "    lstm_model=create_LSTM(n_units,input_shape,output_dim,filepath)\n",
    "    lstm_model.fit(train_x,train_y,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "\n",
    "    # æ¨¡å‹ä¿å­˜\n",
    "    lstm_model.save(model_save_path)\n",
    "\n",
    "    # æµ‹è¯•æ¡æ•°\n",
    "    N= test_x.shape[0]\n",
    "    predict=[]\n",
    "    label=[]\n",
    "    for start,end in zip(range(0,N,1),range(1,N+1,1)):\n",
    "        print(f'start:{start}, end:{end}')\n",
    "        sentence=[inverse_word_dictionary[i] for i in test_x[start] if i!=0]\n",
    "        y_predict=lstm_model.predict(test_x[start:end])\n",
    "        print('y_predict:',y_predict)\n",
    "        label_predict=output_dictionary[np.argmax(y_predict[0])]\n",
    "        label_true=output_dictionary[np.argmax(test_y[start:end])]\n",
    "        print(f'label_predict:{label_predict}, label_true:{label_true}')\n",
    "        # è¾“å‡ºé¢„æµ‹ç»“æœ\n",
    "        print(''.join(sentence),label_true,label_predict)\n",
    "        predict.append(label_predict)\n",
    "        label.append(label_true)\n",
    "\n",
    "    # é¢„æµ‹å‡†ç¡®ç‡\n",
    "    acc=accuracy_score(predict,label)\n",
    "    print('æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡:%s'%acc)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    filepath='C:\\\\æ•°æ®é›†\\\\æƒ…æ„Ÿåˆ†æ60000\\\\all.csv'\n",
    "    input_shape=180\n",
    "    model_save_path='C:\\\\æ•°æ®é›†\\\\æƒ…æ„Ÿåˆ†æ60000\\\\corpus_model.h5'\n",
    "    model_train(input_shape,filepath,model_save_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

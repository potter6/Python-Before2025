{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT模型训练说明\n",
    "模型基于BERT-chinse-base进行finetune\n",
    "* 请在根目录下创建目标目录（若已创建，则跳过）\n",
    "* 请将此notebook另存到上一步所创建的目录下\n",
    "* 请初始化相关路径变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2.1\n"
     ]
    }
   ],
   "source": [
    "! jupyter notebook --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\ML\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.13.1+cu116\n",
      "transformers version: 4.33.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'My_Model'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import transformers\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import trange, notebook\n",
    "from tqdm import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f'torch version: {torch.__version__}\\ntransformers version: {transformers.__version__}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model/bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "model_name = 'bert-base-chinese'\n",
    "# 下面三个文件的路径为 bert-base-chinese 文件夹，根据自己的存储路径更换\n",
    "# 下载地址 https://mirrors.aliyun.com/huggingface/models/bert-base-chinese/\n",
    "config = BertConfig.from_pretrained('model/'+model_name, finetuning_task='binary')  # BERT 模型配置\n",
    "tokenizer = BertTokenizer.from_pretrained('model/'+model_name)  # BERT 的分词器\n",
    "model = BertForSequenceClassification.from_pretrained('model/'+model_name, num_labels=2)  # BERT 的文本分类模型\n",
    " \n",
    "# 用于将文本转换为BERT模型的输入标记\n",
    "def get_tokens(text, tokenizer, max_seq_length, add_special_tokens=True): \n",
    "    # 使用分词器将文本转换为模型可以接受的输入格式\n",
    "    input_ids = tokenizer.encode(text,\n",
    "                                 add_special_tokens=add_special_tokens,\n",
    "                                 truncation=True,\n",
    "                                 max_length=max_seq_length,\n",
    "                                 pad_to_max_length=True)\n",
    "    # 创建一个关注掩码，标记哪些标记是真实文本标记\n",
    "    attention_mask = [int(id > 0) for id in input_ids]\n",
    "    # 确保输入标记和关注掩码的长度等于最大序列长度\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(attention_mask) == max_seq_length\n",
    "    return (input_ids, attention_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入标注数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 目标：保留话题的文字、移除poi超链接、视频超链接、保留表情图片中的标题文字、移除其他html标签\n",
    "def prepare(text):\n",
    "    import re\n",
    "    # 删除包含 wbicon 的 <i> 标签以及它们之间的内容\n",
    "    tokens = re.sub(re.compile(r'(<i\\s+.*?wbicon.*?>.*?</i>)', re.S), '', text)\n",
    "    # 删除与 HTML 匹配的标签，包括尖括号 < 和 > 之间的内容\n",
    "    tokens = re.sub(re.compile(r'<(.*?)>', re.S), '', text)\n",
    "    # 保留 <img title=\"xxx\"> 中的 title 信息\n",
    "    tokens = re.sub(re.compile(r'<img.*?(alt=[\"|\\']{0,1}(.*?)[\"|\\']{0,1}|title=[\"|\\']{0,1}(.*?)[\"|\\']{0,1})\\s+.*?>', re.S|re.M), '\\g<2>', text).strip()\n",
    "    # 将匹配到的 <br/> 替换为 \\n\n",
    "    tokens = re.sub(re.compile(r'<br/>',re.S),'\\n',text)\n",
    "    # 移除文本内的连续重复内容\n",
    "    # 移除连续发生3次及以上次数的重复性内容\n",
    "    # 重复内容的字符串长度>=3\n",
    "    tokens = re.sub(re.compile(r'([\\s|\\S]{2,}?)\\1{2,}',re.S|re.M),'\\g<1>',text)\n",
    "    # 移除 poi链接、视频链接、直播链接\n",
    "    urls=re.findall(r\"<a.*?href=.*?<\\/a>\", text, re.I|re.S|re.M)\n",
    "    url=[u for u in urls if '>2<' in u or 'location_default.png' in u or '视频</a>' in u or '视频</span></a>' in u or '直播</a>' in u]\n",
    "    if len(url)>0:\n",
    "        for u in url:\n",
    "            tokens=text.replace(u,'')\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "# # 对content列进行清洗 由对应的地名在文本中去除content文本中的内容\n",
    "# data['clean_content']=data.apply(lambda row: clean_content(row['content'],row['content_location_name']),axis=1)\n",
    "\n",
    "# # 如果文本为空 去掉该行\n",
    "# data=data[data['clean_content']!='']\n",
    "\n",
    "# # 对清洗后的文本 进行 分词 放入jieba_cut列中\n",
    "# data['jieba_cut']=data['clean_content'].apply(lambda x: jieba.lcut(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       " 1    7597\n",
       " 6    4945\n",
       " 0    2768\n",
       "-1     683\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "def clean_content(content):\n",
    "    import re\n",
    "    # 去除一些关键词\n",
    "    content =content.replace('分享图片', '').replace('分享视频', '').replace('微博视频', '').replace('的微博视频', '').replace('网页链接','').replace('超话','').replace('br','')\n",
    "    # 去除两个#和他们之间的文字\n",
    "    content = re.sub(r'#.*#', '', content)\n",
    "    # 去除@和他们之间的文字\n",
    "    content = re.sub(r'@.*@', '', content)\n",
    "    # 去除[]和他们之间的文字\n",
    "    content = re.sub(r'\\[.*?\\]', '', content)\n",
    "    # 保留表情符号，去除其他符号\n",
    "    content = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0]', '', content)\n",
    "    # 去除英文\n",
    "    content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "    content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.', '').replace('_','')\n",
    "    return content\n",
    "\n",
    "\n",
    "pandarallel.initialize(nb_workers=psutil.cpu_count(logical=False))\n",
    "\n",
    "df_label = pd.read_csv('data\\WeiboTrainData\\weibo_label.csv')\n",
    "\n",
    "df_label['message'] = df_label['message'].parallel_apply(clean_content)\n",
    "\n",
    "df_label.sentiment.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 垃圾文本预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_label.copy()\n",
    "df.loc[df[df.sentiment!=6].index,'sentiment'] = 1\n",
    "\n",
    "df = df[(df['sentiment']==1)|(df['sentiment']==6)]\n",
    "# 随机丢弃标注列表中量较多的数据，以保持二者的标注量基本相同，提高后期模型预测的准确率\n",
    "drop_size = len(df[df['sentiment']==1].sentiment)-len(df[df['sentiment']==6].sentiment)\n",
    "df.drop(df[df['sentiment']==1].sample(drop_size).index, inplace=True)\n",
    "\n",
    "df.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[df.sentiment==6].index,'sentiment'] = 0\n",
    "df.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>4.820000e+15</td>\n",
       "      <td>星期五阴今天的午餐爆炒花甲红烧带鱼小白菜鸡蛋汤</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15676</th>\n",
       "      <td>4.230000e+15</td>\n",
       "      <td>说不害怕是假的每时每刻心里都在难受疑神疑鬼两个性格太过相近的人真的很难太累了有一千零一种方式...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7070</th>\n",
       "      <td>4.230000e+15</td>\n",
       "      <td>分享一则小甜蜜我见哥哥的时候他和我打闹我对着他说没惹我哈我现在还有人追呢哥哥说哪个哪个追你我...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10415</th>\n",
       "      <td>4.230000e+15</td>\n",
       "      <td>有的时候啊我就很想爆粗口听个网易云点开评论劳资觉得好听还不能拿自己账号评论了评论就是混圈滚犊...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>4.230000e+15</td>\n",
       "      <td>整个人都无语了原来有那么人都是小公主还要等着哄吗恶心</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15647</th>\n",
       "      <td>4.230000e+15</td>\n",
       "      <td>说了再也不看青春感伤剧还是不得已被拉了去看虽然男主是渣男但还是有点难受以后再也不要跟一个到感...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15448</th>\n",
       "      <td>4.230000e+15</td>\n",
       "      <td>真的觉得那种在微信上跟你说着什么事拖拖拉拉半天确定不下来一会消失一会出现的人是需要别人好好教...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14088</th>\n",
       "      <td>4.230000e+15</td>\n",
       "      <td>就不多说操作策略是什么了说了也把我的话当耳边风非要反着跟我去操作我都无语了每次我提示大家获利...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14039</th>\n",
       "      <td>4.230000e+15</td>\n",
       "      <td>哪位高人能指点一下对付那种欠钱不还的人该怎么办欠我万元钱明明自己手头有钱家里还是做生意的开着...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15447</th>\n",
       "      <td>4.230000e+15</td>\n",
       "      <td>我真的烦死某些人干的优越感了是是是你们平胸又矮裙子就都该做成超高腰款我们大胸不配穿裙子好吧不...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mid                                            message  \\\n",
       "4261   4.820000e+15                            星期五阴今天的午餐爆炒花甲红烧带鱼小白菜鸡蛋汤   \n",
       "15676  4.230000e+15  说不害怕是假的每时每刻心里都在难受疑神疑鬼两个性格太过相近的人真的很难太累了有一千零一种方式...   \n",
       "7070   4.230000e+15  分享一则小甜蜜我见哥哥的时候他和我打闹我对着他说没惹我哈我现在还有人追呢哥哥说哪个哪个追你我...   \n",
       "10415  4.230000e+15  有的时候啊我就很想爆粗口听个网易云点开评论劳资觉得好听还不能拿自己账号评论了评论就是混圈滚犊...   \n",
       "13995  4.230000e+15                         整个人都无语了原来有那么人都是小公主还要等着哄吗恶心   \n",
       "15647  4.230000e+15  说了再也不看青春感伤剧还是不得已被拉了去看虽然男主是渣男但还是有点难受以后再也不要跟一个到感...   \n",
       "15448  4.230000e+15  真的觉得那种在微信上跟你说着什么事拖拖拉拉半天确定不下来一会消失一会出现的人是需要别人好好教...   \n",
       "14088  4.230000e+15  就不多说操作策略是什么了说了也把我的话当耳边风非要反着跟我去操作我都无语了每次我提示大家获利...   \n",
       "14039  4.230000e+15  哪位高人能指点一下对付那种欠钱不还的人该怎么办欠我万元钱明明自己手头有钱家里还是做生意的开着...   \n",
       "15447  4.230000e+15  我真的烦死某些人干的优越感了是是是你们平胸又矮裙子就都该做成超高腰款我们大胸不配穿裙子好吧不...   \n",
       "\n",
       "       sentiment  \n",
       "4261           1  \n",
       "15676          0  \n",
       "7070           1  \n",
       "10415          0  \n",
       "13995          0  \n",
       "15647          0  \n",
       "15448          0  \n",
       "14088          0  \n",
       "14039          0  \n",
       "15447          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df['message'],  # 文本消息数据\n",
    "                                                    df['sentiment'],  # 文本情感标签\n",
    "                                                    test_size=0.2,    # 测试集占总数据的比例\n",
    "                                                    random_state=42,  # 随机种子，以确保可重复性\n",
    "                                                    stratify=df['sentiment'])  # 根据情感标签进行分层抽样\n",
    "# 使用自定义函数 get_tokens 对训练集和测试集的文本进行分词，每个文本最多包含150个标记\n",
    "X_train_tokens = X_train.apply(get_tokens, args=(tokenizer, 150))\n",
    "X_test_tokens = X_test.apply(get_tokens, args=(tokenizer, 150))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练集的文本特征转换为PyTorch张量\n",
    "input_ids_train = torch.tensor(\n",
    "    [features[0] for features in X_train_tokens.values], dtype=torch.long)  # 输入特征 ID\n",
    "input_mask_train = torch.tensor(\n",
    "    [features[1] for features in X_train_tokens.values], dtype=torch.long)  # 输入掩码\n",
    "label_ids_train = torch.tensor(Y_train.values, dtype=torch.long)  # 标签 ID\n",
    "\n",
    "# # 输出训练集张量的形状\n",
    "# print(input_ids_train.shape)  # 输出训练集输入特征的形状\n",
    "# print(input_mask_train.shape)  # 输出训练集输入掩码的形状\n",
    "# print(label_ids_train.shape)  # 输出训练集标签的形状\n",
    "\n",
    "# 创建训练数据集\n",
    "train_dataset = TensorDataset(input_ids_train, input_mask_train, label_ids_train)\n",
    "\n",
    "# 将测试集的文本特征转换为PyTorch张量\n",
    "input_ids_test = torch.tensor([features[0] for features in X_test_tokens.values], dtype=torch.long)\n",
    "input_mask_test = torch.tensor([features[1] for features in X_test_tokens.values], dtype=torch.long)\n",
    "label_ids_test = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "\n",
    "# 创建测试数据集\n",
    "test_dataset = TensorDataset(input_ids_test, input_mask_test, label_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数量 = 7912\n",
      "训练周期数 = 3\n",
      "总的训练批次大小 = 64\n",
      "总的优化步数 = 41\n"
     ]
    }
   ],
   "source": [
    "# 训练批次大小和训练周期数\n",
    "# train_batch_size = 64\n",
    "train_batch_size = 16\n",
    "num_train_epochs = 3\n",
    "\n",
    "# 创建训练数据采样器和数据加载器\n",
    "train_sampler = RandomSampler(train_dataset)  # 随机采样器，用于随机选择训练样本\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              sampler=train_sampler, \n",
    "                              batch_size=train_batch_size)  # 创建训练数据加载器\n",
    "t_total = len(train_dataloader) // num_train_epochs  # 计算总的训练步数\n",
    "\n",
    "# 输出一些训练相关的信息\n",
    "print(\"样本数量 =\", len(train_dataset))  # 输出训练集样本数量\n",
    "print(\"训练周期数 =\", num_train_epochs)  # 输出训练周期数\n",
    "print(\"总的训练批次大小 =\", train_batch_size)  # 输出总的训练批次大小\n",
    "print(\"总的优化步数 =\", t_total)  # 输出总的优化步数\n",
    "\n",
    "# 优化器和学习率调度器的设置\n",
    "learning_rate = 5e-5  # 学习率\n",
    "adam_epsilon = 1e-8  # Adam优化器的epsilon值\n",
    "warmup_steps = 0  # 学习率预热步数\n",
    "\n",
    "# 创建AdamW优化器和学习率调度器\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=t_total)  # 创建学习率调度器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import trange, tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 假设 train_dataset 已经定义\n",
    "# train_dataset = ...\n",
    "\n",
    "# 检测是否有GPU可用，如果有则使用GPU，否则使用CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 训练批次大小和训练周期数\n",
    "train_batch_size = 32  # 进一步减小批次大小\n",
    "num_train_epochs = 3\n",
    "\n",
    "# 创建训练数据采样器和数据加载器\n",
    "train_sampler = RandomSampler(train_dataset)  # 随机采样器，用于随机选择训练样本\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              sampler=train_sampler, \n",
    "                              batch_size=train_batch_size)  # 创建训练数据加载器\n",
    "\n",
    "# 计算总的优化步数\n",
    "gradient_accumulation_steps = 8  # 增加梯度累积步数\n",
    "t_total = len(train_dataloader) // (num_train_epochs * gradient_accumulation_steps)\n",
    "\n",
    "# 优化器和学习率调度器的设置\n",
    "learning_rate = 5e-5  # 学习率\n",
    "adam_epsilon = 1e-8  # Adam优化器的epsilon值\n",
    "warmup_steps = 0  # 学习率预热步数\n",
    "\n",
    "# 创建AdamW优化器和学习率调度器\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=t_total)  # 创建学习率调度器\n",
    "\n",
    "# 混合精度训练\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 创建一个描述训练周期的迭代器\n",
    "train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
    "\n",
    "# 将模型置于 train 模式\n",
    "model.train()\n",
    "\n",
    "for epoch in train_iterator:\n",
    "    # 创建一个描述迭代的迭代器\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        # 重置每个迭代开始时的所有梯度\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 将模型和输入数据移到GPU（如果可用）\n",
    "        model.to(device)  # 将模型移到GPU或CPU\n",
    "        cuda = next(model.parameters()).device\n",
    "        batch = tuple(t.to(cuda) for t in batch)  # 将批次数据移到GPU或CPU\n",
    "\n",
    "        # 确定传递给模型的输入\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],      # 输入特征ID\n",
    "            'attention_mask': batch[1], # 输入掩码\n",
    "            'labels': batch[2]         # 标签\n",
    "        }\n",
    "\n",
    "        # 通过模型进行前向传播：输入 -> 模型 -> 输出\n",
    "        with autocast():\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # 反向传播损失，自动计算梯度\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 通过将梯度限制在一定范围内来防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 更新模型参数和学习率\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # 打印当前损失值\n",
    "        print(\"\\r%f\" % loss, end='')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测是否有GPU可用，如果有则使用GPU，否则使用CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建一个描述训练周期的迭代器\n",
    "train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
    "\n",
    "# 将模型置于 train 模式\n",
    "model.train()\n",
    "\n",
    "for epoch in train_iterator:\n",
    "    # 创建一个描述迭代的迭代器\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        # 重置每个迭代开始时的所有梯度\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 将模型和输入数据移到GPU（如果可用）\n",
    "        # torch.cuda.empty_cache()  # 清理GPU缓存\n",
    "        model.to(device)  # 将模型移到GPU或CPU\n",
    "        cuda = next(model.parameters()).device\n",
    "        batch = tuple(t.to(cuda) for t in batch)  # 将批次数据移到GPU或CPU\n",
    "\n",
    "        # 确定传递给模型的输入\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],      # 输入特征ID\n",
    "            'attention_mask': batch[1], # 输入掩码\n",
    "            'labels': batch[2]         # 标签\n",
    "        }\n",
    "\n",
    "        # 通过模型进行前向传播：输入 -> 模型 -> 输出\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 打印当前损失值\n",
    "        print(\"\\r%f\" % loss, end='')\n",
    "\n",
    "        # 反向传播损失，自动计算梯度\n",
    "        loss.backward()\n",
    "\n",
    "        # 通过将梯度限制在一定范围内来防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 更新模型参数和学习率\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('My_Model/weibo-bert-rubbish-model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证模型并评估精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 测试批次大小\n",
    "test_batch_size = 64\n",
    "\n",
    "# 创建测试数据采样器和数据加载器\n",
    "test_sampler = SequentialSampler(test_dataset)  # 顺序采样器，用于顺序选择测试样本\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             sampler=test_sampler, \n",
    "                             batch_size=test_batch_size)  # 创建测试数据加载器\n",
    "\n",
    "# 加载之前保存的预训练模型\n",
    "# model = model.from_pretrained('/outputs')\n",
    "\n",
    "# 初始化预测和实际标签\n",
    "preds = None\n",
    "out_label_ids = None\n",
    "\n",
    "# 将模型置于 eval 模式\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"评估中\"):\n",
    "    # 将模型和输入数据移到GPU（如果可用）\n",
    "    model.to(device)\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # 在 eval 模式下不跟踪任何梯度\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],  # 输入特征ID\n",
    "            'attention_mask': batch[1],  # 输入掩码\n",
    "            'labels': batch[2]  # 标签\n",
    "        }        \n",
    "\n",
    "        # 通过模型进行前向传播\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # 我们得到损失，因为我们提供了标签\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "        # 测试数据集可能包含多个批次的项目\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, \n",
    "                                      inputs['labels'].detach().cpu().numpy(), \n",
    "                                      axis=0)\n",
    "\n",
    "# 计算最终损失、预测和准确度\n",
    "preds = np.argmax(preds, axis=1)  # 获取预测类别\n",
    "acc_score = accuracy_score(preds, out_label_ids)  # 计算准确度\n",
    "f1_score = f1_score(preds, out_label_ids)  # 计算F1分数\n",
    "print ('测试集中的Accuracy分数: ', acc_score)\n",
    "print ('测试集中的F1分数: ', f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_origin = pd.read_csv('D:/Code/Python/社交媒体情感/上海2019_2023年按月分类文件/20191201_20200101 57356 条.csv')\n",
    "df_origin['label'] = 0 #统一初始化为0\n",
    "# 对文本进行清洗\n",
    "df_origin['text'] = df_origin.content.str.replace('\\n',' ')\n",
    "\n",
    "# def clean_content(content,place):\n",
    "#     # 删除地名\n",
    "#     content = content.replace(place,'')\n",
    "#     # 删除特殊字符\n",
    "#     content = content.replace('分享图片', '').replace('分享视频', '').replace('微博视频', '').replace('的微博视频', '').replace('#','').replace('，', ' ').replace(',','').replace('。', ' ').replace('；', ' ').replace('、', ' ').replace('！', ' ').replace('+', ' ').replace('：', ' ').replace('\"', '').replace(\"'\", '').replace('/', ' ').replace(\"|\", ' ').replace('……', '').replace('=','').replace('-','').replace('&','').replace(':','').replace('%','').replace('?','')\n",
    "#     # 去除数字和空格\n",
    "#     content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.','')\n",
    "#     # 去除一些符号\n",
    "#     content = content.replace('_','').replace('','').replace('[', '').replace(']', '').replace('【','').replace('】','').replace('<','').replace('>','').replace('《','').replace('》','').replace('（','').replace('）','').replace('(','').replace(')','')\n",
    "#     # 去掉两个#及中间的字符\n",
    "#     # content = re.sub(r'#.*#', '', content)\n",
    "#     # 把a到z的字母去掉\n",
    "#     content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "#     # 只保留中文\n",
    "#     # content = ''.join(filter(lambda x: '\\u4e00' <= x <= '\\u9fa5', content))\n",
    "#     return content\n",
    "\n",
    "def clean_content(content,place):\n",
    "    import re\n",
    "    # 去除地名\n",
    "    content = content.replace(place, '')\n",
    "    # 去除一些关键词\n",
    "    content =content.replace('分享图片', '').replace('分享视频', '').replace('微博视频', '').replace('的微博视频', '').replace('网页链接','').replace('超话','').replace('br','')\n",
    "    # 去除两个#和他们之间的文字\n",
    "    content = re.sub(r'#.*#', '', content)\n",
    "    # 去除@和他们之间的文字\n",
    "    content = re.sub(r'@.*@', '', content)\n",
    "    # 去除[]和他们之间的文字\n",
    "    content = re.sub(r'\\[.*?\\]', '', content)\n",
    "    # 保留表情符号，去除其他符号\n",
    "    content = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0]', '', content)\n",
    "    # 去除英文\n",
    "    content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "    content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.', '').replace('_','')\n",
    "    return content\n",
    "\n",
    "# 对content列进行清洗 由对应的地名在文本中去除content文本中的内容\n",
    "df_origin['text']=df_origin.apply(lambda row: clean_content(row['text'],row['content_location_name']),axis=1)\n",
    "\n",
    "# 如果文本为空 去掉该行\n",
    "df_origin=df_origin[df_origin['text']!='']\n",
    "\n",
    "print(df_origin)\n",
    "\n",
    "X_pred=df_origin['text']\n",
    "Y_pred=df_origin['label']\n",
    "X_pred_tokens = X_pred.parallel_apply(get_tokens, args=(tokenizer, 150))\n",
    "\n",
    "input_ids_pred = torch.tensor(\n",
    "    [features[0] for features in X_pred_tokens.values], dtype=torch.long)\n",
    "input_mask_pred = torch.tensor(\n",
    "    [features[1] for features in X_pred_tokens.values], dtype=torch.long)\n",
    "label_pred=torch.tensor(Y_pred.values,dtype=torch.long)\n",
    "pred_dataset = TensorDataset(input_ids_pred,input_mask_pred,label_pred)\n",
    "\n",
    "pred_batch_size = 256\n",
    "pred_sampler = SequentialSampler(pred_dataset)\n",
    "pred_dataloader = DataLoader(pred_dataset, \n",
    "                             sampler=pred_sampler, \n",
    "                             batch_size=pred_batch_size)\n",
    "\n",
    "# 调用训练好的模型\n",
    "model = model.from_pretrained('My_Model/weibo-bert-rubbish-model')\n",
    "preds = None\n",
    "model.eval()\n",
    "\n",
    "# 预测\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for batch in tqdm(pred_dataloader, desc=\"Predict\"):\n",
    "    \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        _, logits = outputs[:2]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "prob = torch.nn.functional.softmax(torch.tensor(preds), dim=1)  # 使用softmax函数计算预测的概率分布\n",
    "preds = np.argmax(preds, axis=1)  # 计算每个样本的最终预测类别\n",
    "df_origin['ad_prob'] = [p[1].item() for p in prob]  # 将概率分布的第二列（表示\"1\"类别的概率）添加到DataFrame中\n",
    "df_origin['pred'] = preds  # 将最终的预测类别添加到DataFrame中\n",
    "\n",
    "df_origin.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin = pd.read_csv('data/weibo_origin.csv')\n",
    "df_origin['label'] = 0 #统一初始化为0\n",
    "df_origin['text'] = df_origin.message.str.replace('\\n',' ')\n",
    "print(df_origin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred=df_origin['text']\n",
    "Y_pred=df_origin['label']\n",
    "X_pred_tokens = X_pred.parallel_apply(get_tokens, args=(tokenizer, 150))\n",
    "\n",
    "input_ids_pred = torch.tensor(\n",
    "    [features[0] for features in X_pred_tokens.values], dtype=torch.long)\n",
    "input_mask_pred = torch.tensor(\n",
    "    [features[1] for features in X_pred_tokens.values], dtype=torch.long)\n",
    "label_pred=torch.tensor(Y_pred.values,dtype=torch.long)\n",
    "pred_dataset = TensorDataset(input_ids_pred,input_mask_pred,label_pred)\n",
    "\n",
    "pred_batch_size = 256\n",
    "pred_sampler = SequentialSampler(pred_dataset)\n",
    "pred_dataloader = DataLoader(pred_dataset, \n",
    "                             sampler=pred_sampler, \n",
    "                             batch_size=pred_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用训练好的模型\n",
    "model = model.from_pretrained('My_Model/weibo-bert-rubbish-model')\n",
    "preds = None\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for batch in tqdm(pred_dataloader, desc=\"Predict\"):\n",
    "    \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        _, logits = outputs[:2]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = torch.nn.functional.softmax(torch.tensor(preds), dim=1)  # 使用softmax函数计算预测的概率分布\n",
    "preds = np.argmax(preds, axis=1)  # 计算每个样本的最终预测类别\n",
    "df_origin['ad_prob'] = [p[1].item() for p in prob]  # 将概率分布的第二列（表示\"1\"类别的概率）添加到DataFrame中\n",
    "df_origin['pred'] = preds  # 将最终的预测类别添加到DataFrame中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转移时，转移My_Model 以及weibo_label.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "from pandarallel import pandarallel\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import trange, tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def clean_content(content):\n",
    "    import re    \n",
    "    # 去除一些关键词\n",
    "    content =content.replace('分享图片', '').replace('分享视频', '').replace('微博视频', '').replace('的微博视频', '').replace('网页链接','').replace('超话','').replace('br','')\n",
    "    # 去除两个#和他们之间的文字\n",
    "    content = re.sub(r'#.*#', '', content)\n",
    "    # 去除@和他们之间的文字\n",
    "    content = re.sub(r'@.*@', '', content)\n",
    "    # 去除[]和他们之间的文字\n",
    "    content = re.sub(r'\\[.*?\\]', '', content)\n",
    "    # 保留表情符号，去除其他符号\n",
    "    content = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0]', '', content)\n",
    "    # 去除英文\n",
    "    content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "    content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.', '').replace('_','')\n",
    "    return content\n",
    "\n",
    "# 初始化pandarallel\n",
    "pandarallel.initialize(nb_workers=psutil.cpu_count(logical=False))\n",
    "\n",
    "# 读取数据\n",
    "df_label = pd.read_csv('data\\WeiboTrainData\\weibo_label.csv')\n",
    "\n",
    "# 清洗数据\n",
    "df_label['message'] = df_label['message'].parallel_apply(clean_content)\n",
    "\n",
    "# 去除空的行\n",
    "df_label= df_label[df_label['message'] != '']\n",
    "\n",
    "# 数据预处理\n",
    "df_label.sentiment.value_counts()\n",
    "df = df_label.copy()\n",
    "df.loc[df[df.sentiment!=6].index,'sentiment'] = 1\n",
    "df = df[(df['sentiment']==1) | (df['sentiment']==6)]\n",
    "drop_size = len(df[df['sentiment']==1].sentiment) - len(df[df['sentiment']==6].sentiment)\n",
    "df.drop(df[df['sentiment']==1].sample(drop_size).index, inplace=True)\n",
    "df.loc[df[df.sentiment==6].index,'sentiment'] = 0\n",
    "print(df.sentiment.unique())\n",
    "print(df)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(df['message'], df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['message'], df['sentiment'], test_size=0.7, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# 加载BERT模型和分词器\n",
    "model_name = 'bert-base-chinese'\n",
    "config = BertConfig.from_pretrained('model/' + model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained('model/' + model_name)\n",
    "model = BertForSequenceClassification.from_pretrained('model/' + model_name, num_labels=2)\n",
    "\n",
    "# 定义分词函数\n",
    "def get_tokens(text, tokenizer, max_seq_length, add_special_tokens=True):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=add_special_tokens, truncation=True, max_length=max_seq_length, pad_to_max_length=True)\n",
    "    attention_mask = [int(id > 0) for id in input_ids]\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(attention_mask) == max_seq_length\n",
    "    return (input_ids, attention_mask)\n",
    "\n",
    "# 对训练集和测试集进行分词\n",
    "X_train_tokens = X_train.apply(get_tokens, args=(tokenizer, 150))\n",
    "X_test_tokens = X_test.apply(get_tokens, args=(tokenizer, 150))\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "input_ids_train = torch.tensor([features[0] for features in X_train_tokens.values], dtype=torch.long)\n",
    "input_mask_train = torch.tensor([features[1] for features in X_train_tokens.values], dtype=torch.long)\n",
    "label_ids_train = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "\n",
    "input_ids_test = torch.tensor([features[0] for features in X_test_tokens.values], dtype=torch.long)\n",
    "input_mask_test = torch.tensor([features[1] for features in X_test_tokens.values], dtype=torch.long)\n",
    "label_ids_test = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = TensorDataset(input_ids_train, input_mask_train, label_ids_train)\n",
    "test_dataset = TensorDataset(input_ids_test, input_mask_test, label_ids_test)\n",
    "\n",
    "# 训练参数\n",
    "train_batch_size = 64\n",
    "num_train_epochs = 3\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
    "t_total = len(train_dataloader) // num_train_epochs\n",
    "\n",
    "# 输出一些训练相关的信息\n",
    "print(\"样本数量 =\", len(train_dataset))  # 输出训练集样本数量\n",
    "print(\"训练周期数 =\", num_train_epochs)  # 输出训练周期数\n",
    "print(\"总的训练批次大小 =\", train_batch_size)  # 输出总的训练批次大小\n",
    "print(\"总的优化步数 =\", t_total)  # 输出总的优化步数\n",
    "\n",
    "# 优化器和学习率调度器\n",
    "learning_rate = 5e-5\n",
    "adam_epsilon = 1e-8\n",
    "warmup_steps = 0\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "# 设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import csv\n",
    "\n",
    "# 训练模型\n",
    "model.train()\n",
    "train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    # 创建csv文件并写入表头\n",
    "    with open('loss.csv', 'a', newline='') as csvfile:\n",
    "        fieldnames = ['epoch', 'step', 'loss']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.zero_grad()\n",
    "            model.to(device)\n",
    "            cuda=next(model.parameters()).device\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                'input_ids': batch[0], # 输入特征ID\n",
    "                'attention_mask': batch[1],  # 输入掩码\n",
    "                'labels': batch[2]} # 标签\n",
    "            # 通过模型进行 前向传播：输入->模型->输出\n",
    "            outputs = model(**inputs)\n",
    "            # 计算损失\n",
    "            loss = outputs[0]\n",
    "\n",
    "        \n",
    "            # 在循环内部保存epoch、step和loss到csv文件\n",
    "            writer.writerow({'epoch': epoch, 'step': step, 'loss': loss.item()})\n",
    "\n",
    "\n",
    "            # 打印损失值\n",
    "            print(\"\\r%f\" % loss,end='')\n",
    "\n",
    "            # 反响传播损失，自动计算梯度\n",
    "            loss.backward()\n",
    "        \n",
    "            # 通过将梯度限制在一定范围内来防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # 更新模型参数和学习率\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained('My_Model/weibo-bert-rubbish-model')\n",
    "\n",
    "# 评估模型\n",
    "test_batch_size = 64\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)\n",
    "\n",
    "model.eval()\n",
    "preds = None\n",
    "out_label_ids = None\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"评估中\"):\n",
    "    model.to(device)\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = np.argmax(preds, axis=1)\n",
    "acc_score = accuracy_score(preds, out_label_ids)\n",
    "f1_score = f1_score(preds, out_label_ids)\n",
    "print('测试集中的Accuracy分数: ', acc_score)\n",
    "print('测试集中的F1分数: ', f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取数据\n",
    "# df_origin = pd.read_csv(r'D:/Code/Python/senetiment/data/上海2019_2023年按月分类文件/20191201_20200101 57356 条.csv')\n",
    "df_origin = pd.read_csv(r'D:/Code/Python/社交媒体情感/data/上海2019_2023年按月分类文件/20200201_20200301 27808 条.csv')\n",
    "# 只取三行数据\n",
    "# df_origin = df_origin.head(100)\n",
    "df_origin['label'] = 0  # 统一初始化为0\n",
    "\n",
    "# 对文本进行清洗\n",
    "df_origin['text'] = df_origin.content.str.replace('\\n', ' ')\n",
    "\n",
    "def clean_content(content,place):\n",
    "    import re\n",
    "    # 去除地名\n",
    "    content = content.replace(place, '')\n",
    "    # 去除一些关键词\n",
    "    content =content.replace('分享图片', '').replace('分享视频', '').replace('微博视频', '').replace('的微博视频', '').replace('网页链接','').replace('超话','').replace('br','')\n",
    "    # 去除两个#和他们之间的文字\n",
    "    content = re.sub(r'#.*#', '', content)\n",
    "    # 去除@和他们之间的文字\n",
    "    content = re.sub(r'@.*@', '', content)\n",
    "    # 去除[]和他们之间的文字\n",
    "    content = re.sub(r'\\[.*?\\]', '', content)\n",
    "    # 保留表情符号，去除其他符号\n",
    "    content = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0]', '', content)\n",
    "    # 去除英文\n",
    "    content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "    content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.', '').replace('_','')\n",
    "    return content\n",
    "\n",
    "# 对content列进行清洗 由对应的地名在文本中去除content文本中的内容\n",
    "df_origin['text'] = df_origin.apply(lambda row: clean_content(row['text'], row['content_location_name']), axis=1)\n",
    "\n",
    "# 如果文本为空 去掉该行\n",
    "df_origin = df_origin[df_origin['text'] != '']\n",
    "\n",
    "print(df_origin)\n",
    "\n",
    "X_pred = df_origin['text']\n",
    "Y_pred = df_origin['label']\n",
    "\n",
    "# 加载BERT模型和分词器\n",
    "model_name = 'bert-base-chinese'\n",
    "# tokenizer = BertTokenizer.from_pretrained('My_Model/weibo-bert-rubbish-model')\n",
    "model = model.from_pretrained('My_Model/weibo-bert-rubbish-model')\n",
    "# model = BertForSequenceClassification.from_pretrained('model/' + model_name)\n",
    "# tokenizer = BertTokenizer.from_pretrained('model/' + model_name)\n",
    "\n",
    "# 定义分词函数\n",
    "def get_tokens(text, tokenizer, max_seq_length, add_special_tokens=True):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=add_special_tokens, truncation=True, max_length=max_seq_length, pad_to_max_length=True)\n",
    "    attention_mask = [int(id > 0) for id in input_ids]\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(attention_mask) == max_seq_length\n",
    "    return (input_ids, attention_mask)\n",
    "\n",
    "X_pred_tokens = X_pred.apply(get_tokens, args=(tokenizer, 150))\n",
    "\n",
    "input_ids_pred = torch.tensor([features[0] for features in X_pred_tokens.values], dtype=torch.long)\n",
    "input_mask_pred = torch.tensor([features[1] for features in X_pred_tokens.values], dtype=torch.long)\n",
    "label_pred = torch.tensor(Y_pred.values, dtype=torch.long)\n",
    "pred_dataset = TensorDataset(input_ids_pred, input_mask_pred, label_pred)\n",
    "\n",
    "pred_batch_size = 256\n",
    "pred_sampler = SequentialSampler(pred_dataset)\n",
    "pred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size=pred_batch_size)\n",
    "\n",
    "# 预测\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "preds = None\n",
    "for batch in tqdm(pred_dataloader, desc=\"Predict\"):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        _, logits = outputs[:2]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "prob = torch.nn.functional.softmax(torch.tensor(preds), dim=1)  # 使用softmax函数计算预测的概率分布\n",
    "preds = np.argmax(preds, axis=1)  # 计算每个样本的最终预测类别\n",
    "df_origin['ad_prob'] = [p[1].item() for p in prob]  # 将概率分布的第二列（表示\"1\"类别的概率）添加到DataFrame中\n",
    "df_origin['pred'] = preds  # 将最终的预测类别添加到DataFrame中\n",
    "\n",
    "# print(df_origin.sample(10))\n",
    "# df_origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df_origin.to_csv('data\\WeiboTrainData\\sentiment_analysis_data_{}_3.csv'.format(str(datetime.datetime.now()).split(' ')[0]), index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集中的Accuracy分数:  0.7201365187713311\n",
      "测试集中的F1分数:  0.6870229007633589\n"
     ]
    }
   ],
   "source": [
    "print('测试集中的Accuracy分数: ', acc_score)\n",
    "print('测试集中的F1分数: ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "box_positions = [0.5 - 0.1, 0.5+ 0.1]  # 调整位置，使两个箱线图并排显示\n",
    "box_width = 0.2\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "\n",
    "# 绘制箱线图\n",
    "box1 = ax.boxplot(df_origin['ad_prob'], widths=box_width, patch_artist=True,\n",
    "                  boxprops=dict(facecolor='#1f77b4', alpha=0.7),\n",
    "                  medianprops=dict(color='#ff7f0e'),  # 设置中位数线的颜色\n",
    "                  whiskerprops=dict(color='#ff7f0e'))  # 设置须的颜色\n",
    "# box2 = ax.boxplot(data['snownlp+emoji'], positions=[box_positions[1]], widths=box_width, patch_artist=True,\n",
    "#                   boxprops=dict(facecolor='#ff7f0e', alpha=0.7),\n",
    "#                   medianprops=dict(color='#1f77b4'),  # 设置中位数线的颜色\n",
    "#                   whiskerprops=dict(color='#1f77b4'))  # 设置须的颜色\n",
    "\n",
    "# 绘制琴谱图\n",
    "violin1 = ax.violinplot(df_origin['ad_prob'], widths=box_width, showmedians=False)\n",
    "# violin2 = ax.violinplot(data['snownlp+emoji'], positions=[box_positions[1]], widths=box_width, showmedians=False)\n",
    "\n",
    "# 设置琴谱图的颜色和透明度\n",
    "for violin, color in zip([violin1], ['#1f77b4', '#ff7f0e']):\n",
    "    for pc in violin['bodies']:\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_alpha(0.7)\n",
    "\n",
    "# 设置 x 轴标签\n",
    "# ax.set_xticks([0.5])\n",
    "ax.set_xticklabels(['BERT'])\n",
    "\n",
    "# 设置标题和轴标签\n",
    "# ax.set_title('SnowNLP加emoji')\n",
    "ax.set_xlabel('方法')\n",
    "ax.set_ylabel('情绪值')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('data\\WeiboTrainData\\weibo_label.csv')\n",
    "# 去掉data message列中只含有 分享图片的列\n",
    "data=data[data['message']!='分享图片']\n",
    "data.to_csv('data\\WeiboTrainData\\weibo_label.csv',index=False,encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

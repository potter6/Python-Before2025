{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26184\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_link</th>\n",
       "      <th>verify_typ</th>\n",
       "      <th>weibo_link</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>content</th>\n",
       "      <th>image_urls</th>\n",
       "      <th>video_preview_url</th>\n",
       "      <th>forward_num</th>\n",
       "      <th>comment_num</th>\n",
       "      <th>like_num</th>\n",
       "      <th>phone_type</th>\n",
       "      <th>content_location_name</th>\n",
       "      <th>content_location_url</th>\n",
       "      <th>wgs_lng</th>\n",
       "      <th>wgs_lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23290</th>\n",
       "      <td>4.355286e+15</td>\n",
       "      <td>31.174976</td>\n",
       "      <td>121.373729</td>\n",
       "      <td>陈起行</td>\n",
       "      <td>https://weibo.com/u/5403378950</td>\n",
       "      <td>没有认证</td>\n",
       "      <td>https://weibo.com/5403378950/Hncegvh1p</td>\n",
       "      <td>2019-03-29 21:00</td>\n",
       "      <td>捷语班新学期首次出游#Alponse Mucha 2上海·上海明珠美术馆</td>\n",
       "      <td>https://wx3.sinaimg.cn/large/005TG1Hogy1g1jypt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>红米Redmi</td>\n",
       "      <td>上海·上海明珠美术馆</td>\n",
       "      <td>https://weibo.com/p/100101B2094551D369A3FF429C</td>\n",
       "      <td>121.369102</td>\n",
       "      <td>31.176855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11840</th>\n",
       "      <td>4.350541e+15</td>\n",
       "      <td>30.894698</td>\n",
       "      <td>121.091459</td>\n",
       "      <td>青涩天空1015035201</td>\n",
       "      <td>https://weibo.com/u/1015035201</td>\n",
       "      <td>没有认证</td>\n",
       "      <td>https://weibo.com/1015035201/HlcO3rpck</td>\n",
       "      <td>2019-03-16 18:48</td>\n",
       "      <td>分享图片 2上海·金山北站 ​​​​</td>\n",
       "      <td>https://wx3.sinaimg.cn/large/3c803541ly1g14tuh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HUAWEI P10 Plus</td>\n",
       "      <td>上海·金山北站</td>\n",
       "      <td>https://weibo.com/p/100101B2094757D069A3FA419F</td>\n",
       "      <td>121.086874</td>\n",
       "      <td>30.896725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mid        lat         lng       user_name  \\\n",
       "23290  4.355286e+15  31.174976  121.373729             陈起行   \n",
       "11840  4.350541e+15  30.894698  121.091459  青涩天空1015035201   \n",
       "\n",
       "                            user_link verify_typ  \\\n",
       "23290  https://weibo.com/u/5403378950       没有认证   \n",
       "11840  https://weibo.com/u/1015035201       没有认证   \n",
       "\n",
       "                                   weibo_link      publish_time  \\\n",
       "23290  https://weibo.com/5403378950/Hncegvh1p  2019-03-29 21:00   \n",
       "11840  https://weibo.com/1015035201/HlcO3rpck  2019-03-16 18:48   \n",
       "\n",
       "                                    content  \\\n",
       "23290  捷语班新学期首次出游#Alponse Mucha 2上海·上海明珠美术馆   \n",
       "11840                    分享图片 2上海·金山北站 ​​​​   \n",
       "\n",
       "                                              image_urls video_preview_url  \\\n",
       "23290  https://wx3.sinaimg.cn/large/005TG1Hogy1g1jypt...               NaN   \n",
       "11840  https://wx3.sinaimg.cn/large/3c803541ly1g14tuh...               NaN   \n",
       "\n",
       "       forward_num  comment_num  like_num       phone_type  \\\n",
       "23290          0.0          1.0      11.0          红米Redmi   \n",
       "11840          0.0          0.0       1.0  HUAWEI P10 Plus   \n",
       "\n",
       "      content_location_name                            content_location_url  \\\n",
       "23290            上海·上海明珠美术馆  https://weibo.com/p/100101B2094551D369A3FF429C   \n",
       "11840               上海·金山北站  https://weibo.com/p/100101B2094757D069A3FA419F   \n",
       "\n",
       "          wgs_lng    wgs_lat  \n",
       "23290  121.369102  31.176855  \n",
       "11840  121.086874  30.896725  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('data/上海2019_2023年按月分类文件/20190301_20190401 26184 条.csv')\n",
    "\n",
    "filename='data/上海2019_2023年按月分类文件_情绪值/20190301_20190401 26184 条_Tencent.csv'\n",
    "\n",
    "print(len(data)) #数据的行数\n",
    "\n",
    "data.sample(2) #随机抽样5条数据\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "腾讯云NLP的情感倾向分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import types\n",
    "from tencentcloud.common import credential\n",
    "from tencentcloud.common.profile.client_profile import ClientProfile\n",
    "from tencentcloud.common.profile.http_profile import HttpProfile\n",
    "from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException\n",
    "from tencentcloud.nlp.v20190408 import nlp_client, models\n",
    "import pandas as pd\n",
    "\n",
    "key=pd.read_csv('data\\key\\TencentSecretKey.csv')\n",
    "SecretId=key['SecretId'][0]\n",
    "SecretKey=key['SecretKey'][0]\n",
    "\n",
    "def tencent_nlp(text):\n",
    "    try:\n",
    "        # 实例化一个认证对象，入参需要传入腾讯云账户 SecretId 和 SecretKey，此处还需注意密钥对的保密\n",
    "        # 代码泄露可能会导致 SecretId 和 SecretKey 泄露，并威胁账号下所有资源的安全性。以下代码示例仅供参考，建议采用更安全的方式来使用密钥，请参见：https://cloud.tencent.com/document/product/1278/85305\n",
    "        # 密钥可前往官网控制台 https://console.cloud.tencent.com/cam/capi 进行获取\n",
    "        # cred = credential.Credential(\"SecretId\", \"SecretKey\")\n",
    "        cred=credential.Credential(SecretId,SecretKey)\n",
    "        # 实例化一个http选项，可选的，没有特殊需求可以跳过\n",
    "        httpProfile = HttpProfile()\n",
    "        httpProfile.endpoint = \"nlp.tencentcloudapi.com\"\n",
    "\n",
    "        # 实例化一个client选项，可选的，没有特殊需求可以跳过\n",
    "        clientProfile = ClientProfile()\n",
    "        clientProfile.httpProfile = httpProfile\n",
    "        # 实例化要请求产品的client对象,clientProfile是可选的\n",
    "        client = nlp_client.NlpClient(cred, \"\", clientProfile)\n",
    "\n",
    "        # 实例化一个请求对象,每个接口都会对应一个request对象\n",
    "        req = models.AnalyzeSentimentRequest()\n",
    "        params = {\n",
    "            \"Text\": text\n",
    "        }\n",
    "        req.from_json_string(json.dumps(params))\n",
    "\n",
    "        # 返回的resp是一个AnalyzeSentimentResponse的实例，与请求对象对应\n",
    "        resp = client.AnalyzeSentiment(req)\n",
    "        # 输出json格式的字符串回包\n",
    "        # print(resp.to_json_string())\n",
    "\n",
    "        result=json.loads(resp.to_json_string())\n",
    "\n",
    "        positive_prob=result['Positive']\n",
    "        neutral_prob=result['Neutral']\n",
    "        negative_prob=result['Negative']\n",
    "        sentiment=result['Sentiment']\n",
    "        # print('正面概率：'+str(positive_prob))\n",
    "        # print('中性概率：'+str(neutral_prob))\n",
    "        # print('负面概率：'+str(negative_prob))\n",
    "        # print('情感倾向：'+str(sentiment))\n",
    "\n",
    "        return positive_prob,neutral_prob,negative_prob,sentiment\n",
    "\n",
    "    except TencentCloudSDKException as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12495170533657074 0.8017178773880005 0.07333046942949295 neutral\n"
     ]
    }
   ],
   "source": [
    "text=\"你好, 我是一名学生, 我很开心, 你呢？\"\n",
    "\n",
    "pos,sen,neu,neg=tencent_nlp(text)\n",
    "print(pos,sen,neu,neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行数据清洗、情绪值提取#############################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def clean_content(content,place):\n",
    "    # 去除地名\n",
    "    content = content.replace(place, '')\n",
    "    # 去除一些关键词\n",
    "    content = content.replace('分享图片', '').replace('分享视频', '').replace('微博视频', '').replace('的微博视频', '').replace('网页链接','').replace('超话','').replace('新浪图片','').replace('<br>','')    \n",
    "    # 去除英文\n",
    "    content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "    content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.', '').replace('_','')\n",
    "    # 去除所有非中文符号\n",
    "    content = re.sub(r'[^\\u4e00-\\u9fa5]', '', content)\n",
    "    # 去除空白字符\n",
    "    content = re.sub(r'\\s+', '', content)\n",
    "    return content\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "# 对content列进行清洗 由对应的地名在文本中去除content文本中的内容\n",
    "data['clean_content']=data.apply(lambda row: clean_content(row['content'],row['content_location_name']),axis=1)\n",
    "print(\"清洗完成\"+str(len(data)))\n",
    "\n",
    "# 如果文本为空 去掉该行\n",
    "data=data[data['clean_content']!='']\n",
    "print(\"去除空白行完成\"+str(len(data)))\n",
    "\n",
    "# 保留文本字数大于3的和小于512的文本   腾讯是200字\n",
    "data=data[(data['clean_content'].apply(lambda x: len(x)<200)) & (data['clean_content'].apply(lambda x: len(x)>3))]\n",
    "# data=data[data['clean_content'].apply(lambda x: len(x)>3)]\n",
    "print(\"保留字数大于3完成\"+str(len(data)))\n",
    "\n",
    "# 对清洗后的文本 进行 分词 放入jieba_cut列中\n",
    "data['jieba_cut']=data['clean_content'].apply(lambda x: jieba.lcut(x))\n",
    "print(\"分词完成\")\n",
    "\n",
    "def process_row(row):\n",
    "    try:\n",
    "        # 如果是nan\n",
    "        # if pd.isna(row['Tecent_positive']):\n",
    "            positive_prob, neutral_prob, negative_prob,sentiment=tencent_nlp(row['clean_content'])\n",
    "            print(datetime.datetime.now(),\"一次\")\n",
    "            # 休息几秒钟 确保正确跑完 \n",
    "            sleep(0.05*random.randint(2,3))\n",
    "            return positive_prob, neutral_prob, negative_prob,sentiment\n",
    "        # else:\n",
    "            # 不做任何操作跳出，下一步\n",
    "            # return row['Tencent_positive'], row['Tecent_neutral'], row['Tencent_negative'], row['Tencent_sentiment']\n",
    "    except:\n",
    "        print(row)\n",
    "        print(row['clean_content'])\n",
    "        print(\"腾讯API调用失败\")\n",
    "        return np.nan,np.nan,np.nan,np.nan\n",
    "    \n",
    "results = data.apply(process_row, axis=1)\n",
    "data['Tencent_positive'], data['Tencent_neutral'], data['Tencent_negative'], data['Tencent_sentiment'] = zip(*results)\n",
    "print(\"情感分析完成\")\n",
    "\n",
    "# 保存\n",
    "# filename='data\\WeiboTrainData\\sentiment_analysis_data(Baidu)_{}_5.csv'.format(str(datetime.datetime.now()).split(' ')[0])\n",
    "with open(filename, 'w', encoding='utf-8-sig', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(data.columns)  # 写入列名\n",
    "    for row in data.itertuples(index=False):\n",
    "        writer.writerow(row)\n",
    "\n",
    "data.head(5)\n",
    "print(\"保存完成\")\n",
    "\n",
    "time_end=time.time()\n",
    "print(filename,\"本次运行总共耗时：\",time_end-time_start)\n",
    "# data.to_csv('data\\WeiboTrainData\\sentiment_analysis_data(Baidu)_{}_5.csv'.format(str(datetime.datetime.now()).split(' ')[0]), index=False,encoding='utf-8-sig')\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "###############################################只需要修改filename即可，确定好datainputpath和dataoutputpath这两个文件夹与该py文件的相对位置\n",
    "filename='20190801_20190901 60340 条.csv'\n",
    "###############################################注意命名格式是否正确 与csv的名字对应好\n",
    "datainputpath='data/上海2019_2023年按月分类文件/'\n",
    "dataoutputpath='data/上海2019_2023年按月分类文件_情绪值/'\n",
    "data=pd.read_csv(datainputpath+filename)\n",
    "tailname='_Baidu.csv' # 处理提取sentiment值后的文件\n",
    "\n",
    "print(len(data)) #数据的行数\n",
    "data.sample(2) #查看2\n",
    "\n",
    "\n",
    "\n",
    "# 百度的Key导入 ################################################################################################\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import pandas as pd \n",
    "key=pd.read_csv('data\\key\\BaiduAccesskey.csv')\n",
    "# key的读取 \n",
    "API_KEY = key['API_KEY'][0]\n",
    "SECRET_KEY = key['SECRET_KEY'][0]\n",
    "\n",
    "def get_access_token():\n",
    "    \"\"\"\n",
    "    使用 AK，SK 生成鉴权签名（Access Token）\n",
    "    :return: access_token，或是None(如果错误)\n",
    "    \"\"\"\n",
    "    url = \"https://aip.baidubce.com/oauth/2.0/token\"\n",
    "    params = {\"grant_type\": \"client_credentials\", \"client_id\": API_KEY, \"client_secret\": SECRET_KEY}\n",
    "    return str(requests.post(url, params=params).json().get(\"access_token\"))\n",
    "\n",
    "def baidu_nlp(text):\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/nlp/v1/sentiment_classify?charset=UTF-8&access_token=\" + get_access_token()\n",
    "    params = {\n",
    "        'access_token': 'your_access_token',\n",
    "        'text': text\n",
    "    }\n",
    "    headers = {'Content-Type': 'application/json','Accept': 'application/json'}\n",
    "        \n",
    "    response = requests.request(\"POST\",url, data=json.dumps(params), headers=headers)\n",
    "    \n",
    "    result = json.loads(response.text)\n",
    "        \n",
    "    result=response.json()\n",
    "\n",
    "    for item in result['items']:\n",
    "        confidence=item['confidence']\n",
    "        negative_prob=item['negative_prob']\n",
    "        positive_prob=item['positive_prob']\n",
    "        sentiment=item['sentiment']\n",
    "    \n",
    "    return confidence, negative_prob, positive_prob,sentiment\n",
    "\n",
    "# 测试\n",
    "text=\"你好, 我是一名学生, 我很开心, 你呢？\"\n",
    "# text=\"谭鸭血\"\n",
    "\n",
    "c,n,p,s=baidu_nlp(text)\n",
    "print(\"confidence\",c,\"\\nnegative_prob\",n,\"\\npositive_prob\",p,\"\\nsentiment\",s)\n",
    "\n",
    "\n",
    "# 进行数据清洗、情绪值提取#############################################################################\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from time import sleep\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_content(content,place):\n",
    "    # 去除地名\n",
    "    content = content.replace(place, '')\n",
    "    # 去除一些关键词\n",
    "    content =content.replace('分享图片', '').replace('分享视频', '').replace('微博视频', '').replace('的微博视频', '').replace('网页链接','').replace('超话','').replace('新浪图片','').replace('<br>','')    \n",
    "    # 去除英文\n",
    "    content = re.sub(r'[a-zA-Z]+', '', content)\n",
    "    content = re.sub(r'\\d+', '', content).replace(' ', '').replace('.', '').replace('_','')\n",
    "    # 去除所有非中文符号\n",
    "    content = re.sub(r'[^\\u4e00-\\u9fa5]', '', content)\n",
    "    # 去除空白字符\n",
    "    content = re.sub(r'\\s+', '', content)\n",
    "    return content\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "# 对content列进行清洗 由对应的地名在文本中去除content文本中的内容\n",
    "data['clean_content']=data.apply(lambda row: clean_content(row['content'],row['content_location_name']),axis=1)\n",
    "print(\"清洗完成\"+str(len(data)))\n",
    "\n",
    "# 如果文本为空 去掉该行\n",
    "data=data[data['clean_content']!='']\n",
    "print(\"去除空白行完成\"+str(len(data)))\n",
    "\n",
    "# 保留文本字数大于3的和小于512的文本   百度是512字、腾讯是200字\n",
    "data=data[(data['clean_content'].apply(lambda x: len(x)<512)) & (data['clean_content'].apply(lambda x: len(x)>3))]\n",
    "print(\"保留字数大于3完成\"+str(len(data)))\n",
    "\n",
    "# 对清洗后的文本 进行 分词 放入jieba_cut列中\n",
    "data['jieba_cut']=data['clean_content'].apply(lambda x: jieba.lcut(x))\n",
    "print(\"分词完成\"+str(len(data)))\n",
    "\n",
    "def process_row(row):\n",
    "    try:\n",
    "        # 如果是nan 这里被标注的if是后续的处理 不用管\n",
    "        # if pd.isna(row['baidu_confidence']):\n",
    "            # positive_prob, sentiment, neutral_prob, negative_prob=baidu_nlp(row['clean_content'])\n",
    "            # print(datetime.datetime.now(),\"一次\")\n",
    "            # 休息几秒钟 确保正确跑完 \n",
    "            sleep(0.04*random.randint(2,3)) # 0.04秒到0.06秒 之间随机休息比较好\n",
    "            # return positive_prob, sentiment, neutral_prob, negative_prob\n",
    "        # else:\n",
    "            # return row['baidu_confidence'], row['baidu_negative'], row['baidu_positive'], row['baidu_sentiment']\n",
    "    except:\n",
    "        print(row)\n",
    "        print(row['clean_content'])\n",
    "        print(\"百度API调用失败\")\n",
    "        return np.nan,np.nan,np.nan,np.nan\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "# results = data.apply(process_row, axis=1)\n",
    "results = data.progress_apply(process_row, axis=1)\n",
    "try: \n",
    "    data['baidu_confidence'], data['baidu_negative'], data['baidu_positive'], data['baidu_sentiment'] = zip(*results)\n",
    "except:\n",
    "    print(\"百度API调用失败，将results保存后退出\")\n",
    "    results.to_csv(filename,index=False,encoding='utf-8-sig')\n",
    "print(\"情感分析完成\")\n",
    "\n",
    "# 保存\n",
    "# with open(filename+tailname, 'w', encoding='utf-8-sig', newline='') as file:\n",
    "with open(dataoutputpath+filename.split('.')[0]+tailname, 'w', encoding='utf-8-sig', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(data.columns)  # 写入列名\n",
    "    for row in data.itertuples(index=False):\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(data.head(5))\n",
    "time_end=time.time()\n",
    "print(filename,\"本次运行总共耗时：\",time_end-time_start)\n",
    "# data.to_csv('data\\WeiboTrainData\\sentiment_analysis_data(Baidu)_{}_5.csv'.format(str(datetime.datetime.now()).split(' ')[0]), index=False,encoding='utf-8-sig')\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
